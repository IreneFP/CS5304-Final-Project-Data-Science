{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   keyword location                                               text  target\nid                                                                            \n1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix as confmat\n",
    "from sklearn.metrics import f1_score as f1\n",
    "# from sklearn.metrics import plot_confusion_matrix as plot_confmat\n",
    "\n",
    "# import re\n",
    "# import string\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import torch\n",
    "import preprocess\n",
    "import dan\n",
    "\n",
    "import gensim\n",
    "# load dataset and preprocess\n",
    "twt = pd.read_csv('train.csv')\n",
    "twt = twt.set_index('id')\n",
    "twt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   keyword location                                               text  \\\nid                                                                       \n1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n4      NaN      NaN             Forest fire near La Ronge Sask. Canada   \n5      NaN      NaN  All residents asked to 'shelter in place' are ...   \n6      NaN      NaN  13,000 people receive #wildfires evacuation or...   \n7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n    target                                            prepped  \nid                                                             \n1        1  [our, deeds, are, the, reason, of, this, earth...  \n4        1      [forest, fire, near, la, ronge, sask, canada]  \n5        1  [all, residents, asked, to, shelter, in, place...  \n6        1  [people, receive, wildfires, evacuation, order...  \n7        1  [just, got, sent, this, photo, from, ruby, ala...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>prepped</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>[all, residents, asked, to, shelter, in, place...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>[people, receive, wildfires, evacuation, order...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "text = twt['text'].to_list()\n",
    "prep_text = [preprocess.processing(i) for i in text]\n",
    "print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "twt['prepped'] = prep_text\n",
    "twt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "twt['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in twt['prepped']], index = twt.index)\n",
    "\n",
    "devtrain_idx = twt.loc[~twt['vec'].isna()].index.tolist()\n",
    "random.shuffle(devtrain_idx)\n",
    "\n",
    "train_p = 0.7\n",
    "train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "train = [(vec, targ) for targ, vec in  zip(twt['target'][train_idx], twt['vec'][train_idx])]\n",
    "dev = [(vec, targ) for targ, vec in  zip(twt['target'][dev_idx], twt['vec'][dev_idx])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1,     1] loss: 0.691\tDev FI: 0.000\n[1,     2] loss: 0.651\tDev FI: 0.272\n[2,     1] loss: 0.621\tDev FI: 0.690\n[2,     2] loss: 0.585\tDev FI: 0.732\n[3,     1] loss: 0.560\tDev FI: 0.713\n[3,     2] loss: 0.542\tDev FI: 0.703\n[4,     1] loss: 0.530\tDev FI: 0.700\n[4,     2] loss: 0.529\tDev FI: 0.717\n[5,     1] loss: 0.523\tDev FI: 0.725\n[5,     2] loss: 0.513\tDev FI: 0.736\n[6,     1] loss: 0.521\tDev FI: 0.737\n[6,     2] loss: 0.500\tDev FI: 0.739\n[7,     1] loss: 0.518\tDev FI: 0.738\n[7,     2] loss: 0.490\tDev FI: 0.742\n[8,     1] loss: 0.514\tDev FI: 0.741\n[8,     2] loss: 0.483\tDev FI: 0.749\n[9,     1] loss: 0.512\tDev FI: 0.752\n[9,     2] loss: 0.474\tDev FI: 0.749\n[10,     1] loss: 0.510\tDev FI: 0.750\n[10,     2] loss: 0.467\tDev FI: 0.749\nFinished Training.\n"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(dan)\n",
    "net = dan.Net()\n",
    "net.train(train, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Performance and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev[0]\n",
    "ys, y_stars = net.get_eval_data(dev)\n",
    "# confmat(ys, y_stars)\n",
    "f1(ys, y_stars)\n",
    "\n",
    "twt['pred'] = pd.Series(y_stars, index = dev_idx)\n",
    "\n",
    "\n",
    "# for error analysis\n",
    "twt[['keyword', 'location', 'text', 'prepped','vec', 'target','pred']].to_csv('error_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3263 tweets read from test.csv\n3263 tweets processed in the test set.\n3263 3263\n"
    }
   ],
   "source": [
    "# create vector of mean of all word vectors\n",
    "mean_vec = np.zeros((300,1))\n",
    "for vec_targ in train:\n",
    "    mean_vec = np.add(mean_vec, vec_targ[0])\n",
    "mean_vec = mean_vec/len(train)\n",
    "\n",
    "# read in test data and preprocess tweets\n",
    "test = pd.read_csv('test.csv')\n",
    "proc_text = [preprocess.processing(i) for i in text]\n",
    "targets = np.zeros((len(test), 1))\n",
    "print('{} tweets read from test.csv'.format(test.shape[0]))\n",
    "\n",
    "\n",
    "# test_data = [(tweet_vec(x, word2vec), y) if for x, y in zip(proc_test, targets)]\n",
    "test['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in test['text'].tolist()], index = test.index)\n",
    "\n",
    "print('{} tweets processed in the test set.'.format(test.shape[0]))\n",
    "\n",
    "test_data = test['vec'].tolist()\n",
    "\n",
    "\n",
    "_, y_stars = net.get_eval_data(test_data, mode = 'test')\n",
    "\n",
    "# create columns for submission data\n",
    "id = test['id'].to_numpy()\n",
    "target = y_stars\n",
    "\n",
    "# create df with submission data and write to csv\n",
    "submission = pd.DataFrame({'id': id, 'target': target})\n",
    "submission.set_index('id').to_csv('submission.csv')\n",
    "\n",
    "# also save analysis csv for reviewing decisions\n",
    "test['prepped'] = proc_text\n",
    "test['pred'] = target\n",
    "test[['keyword', 'location', 'text', 'prepped','pred']].to_csv('test_error_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id', 'pred']].set_index('id').to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Created 7613 bag-of-words vectorizations.\nNaive Bayes training complete.\nNaive Bayes dev performance 0.7500000000000001\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create raw bow text and labels, shuffle\n",
    "bow = twt['text'].to_numpy()\n",
    "labels = twt['target'].to_numpy()\n",
    "\n",
    "shuffle_idx = [x for x in range(len(bow))]\n",
    "random.shuffle(shuffle_idx)\n",
    "\n",
    "bow = bow[shuffle_idx]\n",
    "labels = labels[shuffle_idx]\n",
    "\n",
    "# train bag-of-words\n",
    "count_vect = CountVectorizer()\n",
    "x_bow = count_vect.fit(bow)\n",
    "\n",
    "# create train dev split\n",
    "train_bow = bow[:round(train_p*len(bow))]\n",
    "dev_bow = bow[round(train_p*len(bow)):]\n",
    "train_bow = count_vect.transform(train_bow)\n",
    "dev_bow = count_vect.transform(dev_bow)\n",
    "\n",
    "train_bow_labels = labels[:round(train_p*len(bow))]\n",
    "dev_bow_lables = labels[round(train_p*len(bow)):]\n",
    "\n",
    "print('Created {} bag-of-words vectorizations.'.format(train_bow.shape[0] + dev_bow.shape[0]))\n",
    "clf = MultinomialNB().fit(train_bow, train_bow_labels)\n",
    "print('Naive Bayes training complete.')\n",
    "\n",
    "# get dev performances\n",
    "ys = clf.predict(dev_bow)\n",
    "\n",
    "# dev performance\n",
    "print('Naive Bayes dev performance {}'.format(f1(ys, dev_bow_lables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness Exploration\n",
    "\n",
    "We suspect that location data can be useful for predicting disasters. However, we have a sizable proportion of the data for which the location data is missing. In order to address this issue, we needed to first identify whether the data is missing completely at random, missing at random, or not missing at random. For the data to be MCAR, the mechanism of missingness of the data must be independent of all other observed features of the data. These other observed features, in our case, include the text of the tweet. Reviewing the Twitter policy on tweet-level location data, we found that twitter users must opt in to location services to embed location data in there tweets, as well as intentionally include location data on each desired tweet. This policy gives us an insight into some mechanisms of missingness - users forget that they can include location data, or the intentionally elect not to for some tweets. This second mechanism would preclude categorization as missing completely at random. Furthermore, as users of social media, the authors have firsthand experience with the relationship between the location from which a tweet was sent, and the inclusion of location data. Twitter users may want to inform followers that they are tweeting from an impressive or otherwise unusual location - a famous concert venue, or a historic city, or even the site of a terrible natural disaster - a decision surely not made at random. Being Missing Not a Random, our options for mitigating location data missingness are few. We could drop the data altogether, but decided that appending it to the contents of the tweet would be an acceptable approach that prevents information loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\nExecuting model with hidden layer dimensionality 2\nFinished Training.\nExecuting model with hidden layer dimensionality 27\nFinished Training.\nExecuting model with hidden layer dimensionality 52\nFinished Training.\nExecuting model with hidden layer dimensionality 77\nFinished Training.\nExecuting model with hidden layer dimensionality 102\nFinished Training.\nExecuting model with hidden layer dimensionality 127\nFinished Training.\nExecuting model with hidden layer dimensionality 152\nFinished Training.\nExecuting model with hidden layer dimensionality 177\nFinished Training.\nExecuting model with hidden layer dimensionality 202\nFinished Training.\nExecuting model with hidden layer dimensionality 227\nFinished Training.\nExecuting model with hidden layer dimensionality 252\nFinished Training.\nExecuting model with hidden layer dimensionality 277\nFinished Training.\n7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\nExecuting model with hidden layer dimensionality 2\nFinished Training.\nExecuting model with hidden layer dimensionality 27\nFinished Training.\nExecuting model with hidden layer dimensionality 52\nFinished Training.\nExecuting model with hidden layer dimensionality 77\nFinished Training.\nExecuting model with hidden layer dimensionality 102\nFinished Training.\nExecuting model with hidden layer dimensionality 127\nFinished Training.\nExecuting model with hidden layer dimensionality 152\nFinished Training.\nExecuting model with hidden layer dimensionality 177\nFinished Training.\nExecuting model with hidden layer dimensionality 202\nFinished Training.\nExecuting model with hidden layer dimensionality 227\nFinished Training.\nExecuting model with hidden layer dimensionality 252\nFinished Training.\nExecuting model with hidden layer dimensionality 277\nFinished Training.\n7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\nExecuting model with hidden layer dimensionality 2\nFinished Training.\nExecuting model with hidden layer dimensionality 27\nFinished Training.\nExecuting model with hidden layer dimensionality 52\nFinished Training.\nExecuting model with hidden layer dimensionality 77\nFinished Training.\nExecuting model with hidden layer dimensionality 102\nFinished Training.\nExecuting model with hidden layer dimensionality 127\nFinished Training.\nExecuting model with hidden layer dimensionality 152\nFinished Training.\nExecuting model with hidden layer dimensionality 177\nFinished Training.\nExecuting model with hidden layer dimensionality 202\nFinished Training.\nExecuting model with hidden layer dimensionality 227\nFinished Training.\nExecuting model with hidden layer dimensionality 252\nFinished Training.\nExecuting model with hidden layer dimensionality 277\nFinished Training.\n7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\nExecuting model with hidden layer dimensionality 2\nFinished Training.\nExecuting model with hidden layer dimensionality 27\nFinished Training.\nExecuting model with hidden layer dimensionality 52\nFinished Training.\nExecuting model with hidden layer dimensionality 77\nFinished Training.\nExecuting model with hidden layer dimensionality 102\nFinished Training.\nExecuting model with hidden layer dimensionality 127\nFinished Training.\nExecuting model with hidden layer dimensionality 152\nFinished Training.\nExecuting model with hidden layer dimensionality 177\nFinished Training.\nExecuting model with hidden layer dimensionality 202\nFinished Training.\nExecuting model with hidden layer dimensionality 227\nFinished Training.\nExecuting model with hidden layer dimensionality 252\nFinished Training.\nExecuting model with hidden layer dimensionality 277\nFinished Training.\n7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\nExecuting model with hidden layer dimensionality 2\nFinished Training.\nExecuting model with hidden layer dimensionality 27\nFinished Training.\nExecuting model with hidden layer dimensionality 52\nFinished Training.\nExecuting model with hidden layer dimensionality 77\nFinished Training.\nExecuting model with hidden layer dimensionality 102\nFinished Training.\nExecuting model with hidden layer dimensionality 127\nFinished Training.\nExecuting model with hidden layer dimensionality 152\nFinished Training.\nExecuting model with hidden layer dimensionality 177\nFinished Training.\nExecuting model with hidden layer dimensionality 202\nFinished Training.\nExecuting model with hidden layer dimensionality 227\nFinished Training.\nExecuting model with hidden layer dimensionality 252\nFinished Training.\nExecuting model with hidden layer dimensionality 277\nFinished Training.\n7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\nExecuting model with hidden layer dimensionality 2\nFinished Training.\nExecuting model with hidden layer dimensionality 27\nFinished Training.\nExecuting model with hidden layer dimensionality 52\nFinished Training.\nExecuting model with hidden layer dimensionality 77\nFinished Training.\nExecuting model with hidden layer dimensionality 102\nFinished Training.\nExecuting model with hidden layer dimensionality 127\nFinished Training.\nExecuting model with hidden layer dimensionality 152\nFinished Training.\nExecuting model with hidden layer dimensionality 177\nFinished Training.\nExecuting model with hidden layer dimensionality 202\nFinished Training.\nExecuting model with hidden layer dimensionality 227\nFinished Training.\nExecuting model with hidden layer dimensionality 252\nFinished Training.\nExecuting model with hidden layer dimensionality 277\nFinished Training.\n7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\nExecuting model with hidden layer dimensionality 2\nFinished Training.\nExecuting model with hidden layer dimensionality 27\nFinished Training.\nExecuting model with hidden layer dimensionality 52\nFinished Training.\nExecuting model with hidden layer dimensionality 77\nFinished Training.\nExecuting model with hidden layer dimensionality 102\nFinished Training.\nExecuting model with hidden layer dimensionality 127\nFinished Training.\nExecuting model with hidden layer dimensionality 152\nFinished Training.\nExecuting model with hidden layer dimensionality 177\nFinished Training.\nExecuting model with hidden layer dimensionality 202\nFinished Training.\nExecuting model with hidden layer dimensionality 227\nFinished Training.\nExecuting model with hidden layer dimensionality 252\nFinished Training.\nExecuting model with hidden layer dimensionality 277\nFinished Training.\n"
    }
   ],
   "source": [
    "# hidden Layer dimensions\n",
    "\n",
    "pre_opts = [(1, 0, 0), (1, 1, 0), (1, 0, 1) , (1, 1, 1), (0, 1, 0), (0, 1, 1), (0, 0, 1)]\n",
    "Fs_ls = []\n",
    "for lower, stopwords, stem in pre_opts:\n",
    "    prep_text = [preprocess.processing(i, lower, stopwords, stem) for i in text]\n",
    "    # print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "\n",
    "    twt['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in prep_text], index = twt.index)\n",
    "\n",
    "    devtrain_idx = twt.loc[~twt['vec'].isna()].index.tolist()\n",
    "    random.shuffle(devtrain_idx)\n",
    "\n",
    "    train_p = 0.7\n",
    "    train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "    dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "    train = [(vec, targ) for targ, vec in  zip(twt['target'][train_idx], twt['vec'][train_idx])]\n",
    "    dev = [(vec, targ) for targ, vec in  zip(twt['target'][dev_idx], twt['vec'][dev_idx])]\n",
    "\n",
    "    Fs = []\n",
    "    for hiddenDim in range(2,302,25):\n",
    "        # print('Executing model with hidden layer dimensionality {}'.format(hiddenDim))\n",
    "        net = dan.Net(hiddenDim)\n",
    "        net.train(train, dev, verbose = False)\n",
    "        ys, y_stars = net.get_eval_data(dev)\n",
    "        Fs.append(f1(ys, y_stars))\n",
    "    Fs_ls.append(Fs)\n",
    "\n",
    "results = {i : Fs_ls[i] for i in range(len(Fs_ls))}\n",
    "pd.DataFrame(results).to_csv('crossvalidation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a grid search dev evaluation of the hiddenDim and preprocessing hyperparameters, we found that hiddenDim = 52, lowercasing, and stopword inclusion provided the best results, at F1 = 0.744"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}