{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7613 entries, 1 to 10873\n",
      "Data columns (total 4 columns):\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 297.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                       location  \\\n",
       "id                                              \n",
       "48      ablaze                     Birmingham   \n",
       "49      ablaze  Est. September 2012 - Bristol   \n",
       "50      ablaze                         AFRICA   \n",
       "52      ablaze               Philadelphia, PA   \n",
       "53      ablaze                     London, UK   \n",
       "...        ...                            ...   \n",
       "10830  wrecked                            NaN   \n",
       "10831  wrecked              Vancouver, Canada   \n",
       "10832  wrecked                        London    \n",
       "10833  wrecked                        Lincoln   \n",
       "10834  wrecked                            NaN   \n",
       "\n",
       "                                                    text  target  \n",
       "id                                                                \n",
       "48     @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "49     We always try to bring the heavy. #metal #RT h...       0  \n",
       "50     #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "52                    Crying out for more! Set me ablaze       0  \n",
       "53     On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n",
       "...                                                  ...     ...  \n",
       "10830   @jt_ruff23 @cameronhacker and I wrecked you both       0  \n",
       "10831  Three days off from work and they've pretty mu...       0  \n",
       "10832  #FX #forex #trading Cramer: Iger's 3 words tha...       0  \n",
       "10833  @engineshed Great atmosphere at the British Li...       0  \n",
       "10834  Cramer: Iger's 3 words that wrecked Disney's s...       0  \n",
       "\n",
       "[7552 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix as confmat\n",
    "from sklearn.metrics import f1_score as f1\n",
    "# from sklearn.metrics import plot_confusion_matrix as plot_confmat\n",
    "\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "#load dataset and preprocess\n",
    "twt = pd.read_csv('train.csv')\n",
    "twt = twt.set_index('id')\n",
    "twt.shape\n",
    "print(twt.info())\n",
    "twt.loc[~twt['keyword'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.loc[10844, 'target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "def processing (sentence):\n",
    "    result = sentence.lower() #Lower case \n",
    "    result = re.sub(r'\\d+', '', result) #Removing numbers\n",
    "    result = result.translate(str.maketrans('', '', string.punctuation)) #Remove weird characters\n",
    "    result = result.strip() #Eliminate blanks from begining and end of setences\n",
    "    result = result.split() #Separate into words\n",
    "    result = [w for w in result if not w in stop_words] #Eliminate stop_words\n",
    "    result = [porter.stem(word) for word in result] #Stem Words\n",
    "    return (result)\n",
    "\n",
    "text = list(twt[\"text\"])\n",
    "len(text)\n",
    "\n",
    "prep_text = [processing(i) for i in text]\n",
    "print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "twt['prepped'] = prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prepped</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, crane, hold, bridg, collaps, near...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ariaahrari, thetawniest, control, wild, fire,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[utckm, volcano, hawaii, httptcozdtoydebj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[polic, investig, ebik, collid, car, littl, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[latest, home, raze, northern, california, wil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword location                                               text  \\\n",
       "id                                                                          \n",
       "1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5         NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...       ...      ...                                                ...   \n",
       "10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "10872     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "       target                                            prepped  \n",
       "id                                                                \n",
       "1           1  [deed, reason, earthquak, may, allah, forgiv, us]  \n",
       "4           1       [forest, fire, near, la, rong, sask, canada]  \n",
       "5           1  [resid, ask, shelter, place, notifi, offic, ev...  \n",
       "6           1  [peopl, receiv, wildfir, evacu, order, califor...  \n",
       "7           1  [got, sent, photo, rubi, alaska, smoke, wildfi...  \n",
       "...       ...                                                ...  \n",
       "10869       1  [two, giant, crane, hold, bridg, collaps, near...  \n",
       "10870       1  [ariaahrari, thetawniest, control, wild, fire,...  \n",
       "10871       1         [utckm, volcano, hawaii, httptcozdtoydebj]  \n",
       "10872       1  [polic, investig, ebik, collid, car, littl, po...  \n",
       "10873       1  [latest, home, raze, northern, california, wil...  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twt.loc[twt['keyword'].any()),]\n",
    "print(twt.shape)\n",
    "twt.dropna()\n",
    "print(twt.shape)\n",
    "# twt['prepped'].isna().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "def tweet_vec (tweet, word2vec):\n",
    "    word_vecs = [word2vec.get_vector(w) for w in tweet if w in word2vec.vocab]\n",
    "#     print(tweet)\n",
    "#     print('Number of words: {}'.format(len(word_vecs)))\n",
    "    if len(word_vecs) >= 1:\n",
    "        return np.stack(word_vecs).mean(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "twt['vec'] = pd.Series([tweet_vec(tweet, word2vec) for tweet in twt['prepped']], index = twt.index)\n",
    "\n",
    "devtrain_idx = twt.loc[~twt['vec'].isna()].index.tolist()\n",
    "random.shuffle(devtrain_idx)\n",
    "\n",
    "train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "train = [(vec, targ) for targ, vec in  zip(twt['target'][train_idx], twt['vec'][train_idx])]\n",
    "dev = [(vec, targ) for targ, vec in  zip(twt['target'][dev_idx], twt['vec'][dev_idx])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.691\tDev FI: 0.002\n",
      "[1,     2] loss: 0.650\tDev FI: 0.145\n",
      "[2,     1] loss: 0.636\tDev FI: 0.712\n",
      "[2,     2] loss: 0.591\tDev FI: 0.712\n",
      "[3,     1] loss: 0.576\tDev FI: 0.707\n",
      "[3,     2] loss: 0.538\tDev FI: 0.698\n",
      "[4,     1] loss: 0.552\tDev FI: 0.709\n",
      "[4,     2] loss: 0.515\tDev FI: 0.710\n",
      "[5,     1] loss: 0.541\tDev FI: 0.718\n",
      "[5,     2] loss: 0.503\tDev FI: 0.716\n",
      "[6,     1] loss: 0.538\tDev FI: 0.711\n",
      "[6,     2] loss: 0.489\tDev FI: 0.705\n",
      "[7,     1] loss: 0.537\tDev FI: 0.710\n",
      "[7,     2] loss: 0.477\tDev FI: 0.713\n",
      "[8,     1] loss: 0.535\tDev FI: 0.715\n",
      "[8,     2] loss: 0.467\tDev FI: 0.717\n",
      "[9,     1] loss: 0.532\tDev FI: 0.717\n",
      "[9,     2] loss: 0.459\tDev FI: 0.717\n",
      "[10,     1] loss: 0.530\tDev FI: 0.719\n",
      "[10,     2] loss: 0.450\tDev FI: 0.715\n",
      "Finished Training\n",
      "<generator object Module.parameters at 0x00000203BF5A40C8>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(300,300)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(300, 2)\n",
    "        self.softmax =  nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def get_eval_data(self, data):\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size = 1)\n",
    "\n",
    "        y_stars = []\n",
    "        ys = [vec_targ[1] for vec_targ in data]\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            x, _ = data\n",
    "            # print(x)\n",
    "            output = self.forward(x).detach().numpy()[0]\n",
    "            y_star = np.argmax(output)\n",
    "            #print(y_star)\n",
    "            y_stars.append(y_star)\n",
    "        \n",
    "        return ys, y_stars\n",
    "\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size = 5000)\n",
    "net = Net()\n",
    "# print(net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         print(inputs)\n",
    "#         print(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1 == 0: # print every 2000 mini-batches\n",
    "            ys, y_stars = net.get_eval_data(dev)\n",
    "            print('[%d, %5d] loss: %.3f\\tDev FI: %.3f' % (epoch + 1, i + 1, running_loss, f1(ys, y_stars)))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "print(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Performance and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev[0]\n",
    "ys, y_stars = net.get_eval_data(dev)\n",
    "# confmat(ys, y_stars)\n",
    "f1(ys, y_stars)\n",
    "\n",
    "twt['pred'] = pd.Series(y_stars, index = dev_idx)\n",
    "\n",
    "# for error analysis\n",
    "twt.loc[dev_idx,].to_csv('error_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263 tweets read from test.csv\n",
      "3263 tweets processed in the test set.\n",
      "0 empty tweets replaced with mean vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# create vector of mean of all word vectors\n",
    "mean_vec = np.zeros((300,1))\n",
    "for vec_targ in train:\n",
    "    mean_vec = np.add(mean_vec, vec_targ[0])\n",
    "mean_vec = mean_vec/len(final_data)\n",
    "\n",
    "# read in test data and preprocess tweets\n",
    "test = pd.read_csv('test.csv')\n",
    "text = list(test[\"text\"])\n",
    "proc_text = [processing(i) for i in text]\n",
    "targets = np.zeros((len(test), 1))\n",
    "print('{} tweets read from test.csv'.format(test.shape[0]))\n",
    "\n",
    "test_data = []\n",
    "counter = 0\n",
    "\n",
    "# replace empty vectors with mean_vec\n",
    "for x, y in zip(prep_text, targets):\n",
    "    vec = tweet_vec(x, word2vec)\n",
    "    if vec is not None:\n",
    "        test_data.append((vec, y))\n",
    "    else:\n",
    "        test_data.append((mean_vec, y))\n",
    "        counter += 0\n",
    "        \n",
    "print('{} tweets processed in the test set.'.format(len(test_data)))\n",
    "print('{} empty tweets replaced with mean vector'.format(counter))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 1)\n",
    "\n",
    "y_stars = []\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    x, y = data\n",
    "    #print(x.dtype)\n",
    "    #x_dub = torch.Tensor(x, dtype = 'double')\n",
    "    #print(x.dtype)\n",
    "    output = net.forward(x.float()).detach().numpy()[0]\n",
    "    y_star = np.argmax(output)\n",
    "    #print(y_star)\n",
    "    y_stars.append(y_star)\n",
    "\n",
    "# create columns for submission data\n",
    "id = test['id'].to_numpy()\n",
    "target = y_stars\n",
    "\n",
    "# create df with submission data and write to csv\n",
    "submission = pd.DataFrame({'id': id, 'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.set_index('id')\n",
    "submission.to_csv('submission_3_19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7613 bag-of-words vectorizations.\n",
      "Naive Bayes training complete.\n",
      "Naive Bayes dev performance 0.7468085106382979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create raw bow text and labels, shuffle\n",
    "bow = twt['text'].to_numpy()\n",
    "labels = twt['target'].to_numpy()\n",
    "\n",
    "shuffle_idx = [x for x in range(len(bow))]\n",
    "random.shuffle(shuffle_idx)\n",
    "\n",
    "bow = bow[shuffle_idx]\n",
    "labels = labels[shuffle_idx]\n",
    "\n",
    "# train bag-of-words\n",
    "count_vect = CountVectorizer()\n",
    "x_bow = count_vect.fit(bow)\n",
    "\n",
    "# create train dev split\n",
    "train_bow = bow[:round(train_p*len(bow))]\n",
    "dev_bow = bow[round(train_p*len(bow)):]\n",
    "train_bow = count_vect.transform(train_bow)\n",
    "dev_bow = count_vect.transform(dev_bow)\n",
    "\n",
    "train_bow_labels = labels[:round(train_p*len(bow))]\n",
    "dev_bow_lables = labels[round(train_p*len(bow)):]\n",
    "\n",
    "print('Created {} bag-of-words vectorizations.'.format(train_bow.shape[0] + dev_bow.shape[0]))\n",
    "clf = MultinomialNB().fit(train_bow, train_bow_labels)\n",
    "print('Naive Bayes training complete.')\n",
    "\n",
    "# get dev performances\n",
    "ys = clf.predict(dev_bow)\n",
    "\n",
    "# dev performance\n",
    "print('Naive Bayes dev performance {}'.format(f1(ys, dev_bow_lables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness Exploration\n",
    "\n",
    "We suspect that location data can be useful for predicting disasters. However, we have a sizable proportion of the data for which the location data is missing. In order to address this issue, we needed to first identify whether the data is missing completely at random, missing at random, or not missing at random. For the data to be MCAR, the mechanism of missingness of the data must be independent of all other observed features of the data. These other observed features, in our case, include the text of the tweet. Reviewing the Twitter policy on tweet-level location data, we found that twitter users must opt in to location services to embed location data in there tweets, as well as intentionally include location data on each desired tweet. This policy gives us an insight into some mechanisms of missingness - users forget that they can include location data, or the intentionally elect not to for some tweets. This second mechanism would preclude categorization as missing completely at random. Furthermore, as users of social media, we have firsthand experience with the relationship between the location from which a tweet was sent, and the inclusion of location data. Twitter users may want to inform followers that they are tweeting from an impressive or otherwise unusual location - a famous concert venue, or a historic city, or even the site of a terrible natural disaster - indicating to us a relationship between the location to be included and the likelihood of inclusion. Being Missing Not a Random, our options for mitigating location data missingness are few. We could drop the data altogether, but decided that appending it to the contents of the tweet would be an acceptable approach that prevents information loss.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
