{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    0.57034\n1    0.42966\nName: target, dtype: float64\n0    1.0\nName: target, dtype: float64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       keyword                       location  \\\nid                                              \n48      ablaze                     Birmingham   \n49      ablaze  Est. September 2012 - Bristol   \n50      ablaze                         AFRICA   \n52      ablaze               Philadelphia, PA   \n53      ablaze                     London, UK   \n...        ...                            ...   \n10830  wrecked                            NaN   \n10831  wrecked              Vancouver, Canada   \n10832  wrecked                        London    \n10833  wrecked                        Lincoln   \n10834  wrecked                            NaN   \n\n                                                    text  target  \\\nid                                                                 \n48     @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n49     We always try to bring the heavy. #metal #RT h...       0   \n50     #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n52                    Crying out for more! Set me ablaze       0   \n53     On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   \n...                                                  ...     ...   \n10830   @jt_ruff23 @cameronhacker and I wrecked you both       0   \n10831  Three days off from work and they've pretty mu...       0   \n10832  #FX #forex #trading Cramer: Iger's 3 words tha...       0   \n10833  @engineshed Great atmosphere at the British Li...       0   \n10834  Cramer: Iger's 3 words that wrecked Disney's s...       0   \n\n                                                   text+  \\\nid                                                         \n48     Birmingham @bbcmtd Wholesale Markets ablaze ht...   \n49     Est. September 2012 - Bristol We always try to...   \n50     AFRICA #AFRICANBAZE: Breaking news:Nigeria fla...   \n52     Philadelphia, PA Crying out for more! Set me a...   \n53     London, UK On plus side LOOK AT THE SKY LAST N...   \n...                                                  ...   \n10830   @jt_ruff23 @cameronhacker and I wrecked you both   \n10831  Vancouver, Canada Three days off from work and...   \n10832  London  #FX #forex #trading Cramer: Iger's 3 w...   \n10833  Lincoln @engineshed Great atmosphere at the Br...   \n10834  Cramer: Iger's 3 words that wrecked Disney's s...   \n\n                                                  text++  \nid                                                        \n48     ablaze Birmingham @bbcmtd Wholesale Markets ab...  \n49     ablaze Est. September 2012 - Bristol We always...  \n50     ablaze AFRICA #AFRICANBAZE: Breaking news:Nige...  \n52     ablaze Philadelphia, PA Crying out for more! S...  \n53     ablaze London, UK On plus side LOOK AT THE SKY...  \n...                                                  ...  \n10830  wrecked @jt_ruff23 @cameronhacker and I wrecke...  \n10831  wrecked Vancouver, Canada Three days off from ...  \n10832  wrecked London  #FX #forex #trading Cramer: Ig...  \n10833  wrecked Lincoln @engineshed Great atmosphere a...  \n10834  wrecked Cramer: Iger's 3 words that wrecked Di...  \n\n[7552 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text+</th>\n      <th>text++</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>48</th>\n      <td>ablaze</td>\n      <td>Birmingham</td>\n      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n      <td>1</td>\n      <td>Birmingham @bbcmtd Wholesale Markets ablaze ht...</td>\n      <td>ablaze Birmingham @bbcmtd Wholesale Markets ab...</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>ablaze</td>\n      <td>Est. September 2012 - Bristol</td>\n      <td>We always try to bring the heavy. #metal #RT h...</td>\n      <td>0</td>\n      <td>Est. September 2012 - Bristol We always try to...</td>\n      <td>ablaze Est. September 2012 - Bristol We always...</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>ablaze</td>\n      <td>AFRICA</td>\n      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n      <td>1</td>\n      <td>AFRICA #AFRICANBAZE: Breaking news:Nigeria fla...</td>\n      <td>ablaze AFRICA #AFRICANBAZE: Breaking news:Nige...</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>ablaze</td>\n      <td>Philadelphia, PA</td>\n      <td>Crying out for more! Set me ablaze</td>\n      <td>0</td>\n      <td>Philadelphia, PA Crying out for more! Set me a...</td>\n      <td>ablaze Philadelphia, PA Crying out for more! S...</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>ablaze</td>\n      <td>London, UK</td>\n      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n      <td>0</td>\n      <td>London, UK On plus side LOOK AT THE SKY LAST N...</td>\n      <td>ablaze London, UK On plus side LOOK AT THE SKY...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10830</th>\n      <td>wrecked</td>\n      <td>NaN</td>\n      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n      <td>0</td>\n      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n      <td>wrecked @jt_ruff23 @cameronhacker and I wrecke...</td>\n    </tr>\n    <tr>\n      <th>10831</th>\n      <td>wrecked</td>\n      <td>Vancouver, Canada</td>\n      <td>Three days off from work and they've pretty mu...</td>\n      <td>0</td>\n      <td>Vancouver, Canada Three days off from work and...</td>\n      <td>wrecked Vancouver, Canada Three days off from ...</td>\n    </tr>\n    <tr>\n      <th>10832</th>\n      <td>wrecked</td>\n      <td>London</td>\n      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n      <td>0</td>\n      <td>London  #FX #forex #trading Cramer: Iger's 3 w...</td>\n      <td>wrecked London  #FX #forex #trading Cramer: Ig...</td>\n    </tr>\n    <tr>\n      <th>10833</th>\n      <td>wrecked</td>\n      <td>Lincoln</td>\n      <td>@engineshed Great atmosphere at the British Li...</td>\n      <td>0</td>\n      <td>Lincoln @engineshed Great atmosphere at the Br...</td>\n      <td>wrecked Lincoln @engineshed Great atmosphere a...</td>\n    </tr>\n    <tr>\n      <th>10834</th>\n      <td>wrecked</td>\n      <td>NaN</td>\n      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n      <td>0</td>\n      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n      <td>wrecked Cramer: Iger's 3 words that wrecked Di...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7552 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isnan\n",
    "from sklearn.metrics import confusion_matrix as confmat\n",
    "from sklearn.metrics import f1_score as f1\n",
    "# from sklearn.metrics import plot_confusion_matrix as plot_confmat\n",
    "\n",
    "# import re\n",
    "# import string\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import torch\n",
    "import preprocess\n",
    "import dan\n",
    "\n",
    "import gensim\n",
    "# load dataset and preprocess\n",
    "# twt = pd.read_csv('train.csv')\n",
    "twt = pd.read_csv('train.csv')\n",
    "twt = twt.set_index('id')\n",
    "# twt = twt.loc[twt.index < 20000,]\n",
    "add = pd.read_csv('tweets_0_300.csv')\n",
    "add = add.set_index('id')\n",
    "twt.head()\n",
    "\n",
    "print(twt['target'].value_counts()/len(twt['target']))\n",
    "print(add['target'].value_counts()/len(add['target']))\n",
    "\n",
    "# create text+ column which includes location\n",
    "twt['text+'] = twt.apply(lambda x: x['text'] if pd.isna(x['location']) else  x['location'] + ' ' + x['text'], axis = 1) \n",
    "\n",
    "twt['text++'] = twt.apply(lambda x: x['text+'] if pd.isna(x['keyword']) else x['keyword'] + ' ' + x['text+'], axis = 1)\n",
    "twt.loc[~twt['keyword'].isna(),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      keyword location                                               text  \\\nid                                                                          \n10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n10872     NaN      NaN  Police investigating after an e-bike collided ...   \n10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n\n       target                                              text+  \\\nid                                                                 \n10869       1  Two giant cranes holding a bridge collapse int...   \n10870       1  @aria_ahrary @TheTawniest The out of control w...   \n10871       1  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n10872       1  Police investigating after an e-bike collided ...   \n10873       1  The Latest: More Homes Razed by Northern Calif...   \n\n                                                 prepped  \nid                                                        \n10869  [two, giant, cranes, holding, a, bridge, colla...  \n10870  [ariaahrary, thetawniest, the, out, of, contro...  \n10871  [m, utckm, s, of, volcano, hawaii, httptcozdto...  \n10872  [police, investigating, after, an, ebike, coll...  \n10873  [the, latest, more, homes, razed, by, northern...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text+</th>\n      <th>prepped</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10869</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>[two, giant, cranes, holding, a, bridge, colla...</td>\n    </tr>\n    <tr>\n      <th>10870</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>[ariaahrary, thetawniest, the, out, of, contro...</td>\n    </tr>\n    <tr>\n      <th>10871</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>[m, utckm, s, of, volcano, hawaii, httptcozdto...</td>\n    </tr>\n    <tr>\n      <th>10872</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>[police, investigating, after, an, ebike, coll...</td>\n    </tr>\n    <tr>\n      <th>10873</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>[the, latest, more, homes, razed, by, northern...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "text = twt['text+'].to_list()\n",
    "prep_text = [preprocess.processing(i) for i in text]\n",
    "print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))  \n",
    "twt['prepped'] = prep_text\n",
    "twt.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "importlib.reload(preprocess)\n",
    "twt['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in twt['prepped']], index = twt.index)\n",
    "\n",
    "devtrain_idx = twt.loc[~twt['vec'].isna()].index.tolist()\n",
    "random.shuffle(devtrain_idx)\n",
    "\n",
    "train_p = 0.90\n",
    "train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "train = [(vec, targ) for targ, vec in  zip(twt['target'][train_idx], twt['vec'][train_idx])]\n",
    "dev = [(vec, targ) for targ, vec in  zip(twt['target'][dev_idx], twt['vec'][dev_idx])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1,     1] loss: 0.701\tDev FI: 0.629\n[1,     2] loss: 0.692\tDev FI: 0.353\n[2,     1] loss: 0.683\tDev FI: 0.016\n[2,     2] loss: 0.675\tDev FI: 0.004\n[3,     1] loss: 0.670\tDev FI: 0.000\n[3,     2] loss: 0.662\tDev FI: 0.000\n[4,     1] loss: 0.660\tDev FI: 0.004\n[4,     2] loss: 0.652\tDev FI: 0.006\n[5,     1] loss: 0.652\tDev FI: 0.012\n[5,     2] loss: 0.643\tDev FI: 0.018\n[6,     1] loss: 0.645\tDev FI: 0.045\n[6,     2] loss: 0.635\tDev FI: 0.119\n[7,     1] loss: 0.636\tDev FI: 0.232\n[7,     2] loss: 0.625\tDev FI: 0.329\n[8,     1] loss: 0.625\tDev FI: 0.411\n[8,     2] loss: 0.613\tDev FI: 0.507\n[9,     1] loss: 0.613\tDev FI: 0.571\n[9,     2] loss: 0.602\tDev FI: 0.629\n[10,     1] loss: 0.601\tDev FI: 0.656\n[10,     2] loss: 0.592\tDev FI: 0.686\n[11,     1] loss: 0.591\tDev FI: 0.703\n[11,     2] loss: 0.583\tDev FI: 0.709\n[12,     1] loss: 0.583\tDev FI: 0.711\n[12,     2] loss: 0.575\tDev FI: 0.715\n[13,     1] loss: 0.575\tDev FI: 0.716\n[13,     2] loss: 0.568\tDev FI: 0.715\n[14,     1] loss: 0.568\tDev FI: 0.715\n[14,     2] loss: 0.560\tDev FI: 0.715\n[15,     1] loss: 0.561\tDev FI: 0.713\n[15,     2] loss: 0.552\tDev FI: 0.715\n[16,     1] loss: 0.556\tDev FI: 0.714\n[16,     2] loss: 0.545\tDev FI: 0.713\n[17,     1] loss: 0.552\tDev FI: 0.712\n[17,     2] loss: 0.540\tDev FI: 0.713\n[18,     1] loss: 0.548\tDev FI: 0.712\n[18,     2] loss: 0.535\tDev FI: 0.711\n[19,     1] loss: 0.545\tDev FI: 0.713\n[19,     2] loss: 0.530\tDev FI: 0.718\n[20,     1] loss: 0.541\tDev FI: 0.718\n[20,     2] loss: 0.526\tDev FI: 0.719\n[21,     1] loss: 0.538\tDev FI: 0.720\n[21,     2] loss: 0.522\tDev FI: 0.720\n[22,     1] loss: 0.535\tDev FI: 0.724\n[22,     2] loss: 0.519\tDev FI: 0.726\n[23,     1] loss: 0.533\tDev FI: 0.728\n[23,     2] loss: 0.516\tDev FI: 0.730\n[24,     1] loss: 0.531\tDev FI: 0.730\n[24,     2] loss: 0.513\tDev FI: 0.732\n[25,     1] loss: 0.529\tDev FI: 0.736\n[25,     2] loss: 0.510\tDev FI: 0.737\n[26,     1] loss: 0.527\tDev FI: 0.736\n[26,     2] loss: 0.506\tDev FI: 0.736\n[27,     1] loss: 0.526\tDev FI: 0.738\n[27,     2] loss: 0.503\tDev FI: 0.739\n[28,     1] loss: 0.525\tDev FI: 0.739\n[28,     2] loss: 0.501\tDev FI: 0.740\n[29,     1] loss: 0.524\tDev FI: 0.741\n[29,     2] loss: 0.498\tDev FI: 0.739\n[30,     1] loss: 0.522\tDev FI: 0.741\n[30,     2] loss: 0.495\tDev FI: 0.741\n[31,     1] loss: 0.521\tDev FI: 0.741\n[31,     2] loss: 0.493\tDev FI: 0.743\n[32,     1] loss: 0.520\tDev FI: 0.742\n[32,     2] loss: 0.490\tDev FI: 0.743\n[33,     1] loss: 0.519\tDev FI: 0.745\n[33,     2] loss: 0.488\tDev FI: 0.745\n[34,     1] loss: 0.518\tDev FI: 0.746\n[34,     2] loss: 0.486\tDev FI: 0.745\n[35,     1] loss: 0.517\tDev FI: 0.748\n[35,     2] loss: 0.484\tDev FI: 0.750\n[36,     1] loss: 0.517\tDev FI: 0.749\n[36,     2] loss: 0.481\tDev FI: 0.749\n[37,     1] loss: 0.516\tDev FI: 0.749\n[37,     2] loss: 0.479\tDev FI: 0.751\n[38,     1] loss: 0.515\tDev FI: 0.751\n[38,     2] loss: 0.477\tDev FI: 0.752\n[39,     1] loss: 0.514\tDev FI: 0.754\n[39,     2] loss: 0.476\tDev FI: 0.755\n[40,     1] loss: 0.514\tDev FI: 0.755\n[40,     2] loss: 0.474\tDev FI: 0.755\n[41,     1] loss: 0.513\tDev FI: 0.756\n[41,     2] loss: 0.472\tDev FI: 0.756\n[42,     1] loss: 0.513\tDev FI: 0.755\n[42,     2] loss: 0.470\tDev FI: 0.755\n[43,     1] loss: 0.512\tDev FI: 0.753\n[43,     2] loss: 0.468\tDev FI: 0.753\n[44,     1] loss: 0.512\tDev FI: 0.753\n[44,     2] loss: 0.467\tDev FI: 0.753\n[45,     1] loss: 0.511\tDev FI: 0.754\n[45,     2] loss: 0.465\tDev FI: 0.755\n[46,     1] loss: 0.511\tDev FI: 0.756\n[46,     2] loss: 0.464\tDev FI: 0.754\n[47,     1] loss: 0.510\tDev FI: 0.754\n[47,     2] loss: 0.462\tDev FI: 0.754\n[48,     1] loss: 0.510\tDev FI: 0.753\n[48,     2] loss: 0.461\tDev FI: 0.753\n[49,     1] loss: 0.509\tDev FI: 0.753\n[49,     2] loss: 0.459\tDev FI: 0.753\n[50,     1] loss: 0.509\tDev FI: 0.754\n[50,     2] loss: 0.458\tDev FI: 0.755\n[51,     1] loss: 0.509\tDev FI: 0.754\n[51,     2] loss: 0.457\tDev FI: 0.754\n[52,     1] loss: 0.508\tDev FI: 0.754\n[52,     2] loss: 0.455\tDev FI: 0.754\n[53,     1] loss: 0.508\tDev FI: 0.755\n[53,     2] loss: 0.454\tDev FI: 0.755\n[54,     1] loss: 0.508\tDev FI: 0.755\n[54,     2] loss: 0.453\tDev FI: 0.755\n[55,     1] loss: 0.507\tDev FI: 0.754\n[55,     2] loss: 0.451\tDev FI: 0.755\n[56,     1] loss: 0.507\tDev FI: 0.757\n[56,     2] loss: 0.450\tDev FI: 0.756\n[57,     1] loss: 0.507\tDev FI: 0.757\n[57,     2] loss: 0.449\tDev FI: 0.757\n[58,     1] loss: 0.506\tDev FI: 0.758\n[58,     2] loss: 0.447\tDev FI: 0.757\n[59,     1] loss: 0.506\tDev FI: 0.757\n[59,     2] loss: 0.446\tDev FI: 0.757\n[60,     1] loss: 0.506\tDev FI: 0.757\n[60,     2] loss: 0.445\tDev FI: 0.757\n[61,     1] loss: 0.505\tDev FI: 0.757\n[61,     2] loss: 0.443\tDev FI: 0.757\n[62,     1] loss: 0.505\tDev FI: 0.758\n[62,     2] loss: 0.442\tDev FI: 0.759\n[63,     1] loss: 0.505\tDev FI: 0.759\n[63,     2] loss: 0.441\tDev FI: 0.757\n[64,     1] loss: 0.505\tDev FI: 0.757\n[64,     2] loss: 0.440\tDev FI: 0.757\n[65,     1] loss: 0.504\tDev FI: 0.757\n[65,     2] loss: 0.438\tDev FI: 0.757\n[66,     1] loss: 0.504\tDev FI: 0.757\n[66,     2] loss: 0.437\tDev FI: 0.758\n[67,     1] loss: 0.504\tDev FI: 0.757\n[67,     2] loss: 0.436\tDev FI: 0.757\n[68,     1] loss: 0.504\tDev FI: 0.756\n[68,     2] loss: 0.435\tDev FI: 0.756\n[69,     1] loss: 0.504\tDev FI: 0.756\n[69,     2] loss: 0.434\tDev FI: 0.756\n[70,     1] loss: 0.503\tDev FI: 0.757\n[70,     2] loss: 0.433\tDev FI: 0.756\n[71,     1] loss: 0.503\tDev FI: 0.757\n[71,     2] loss: 0.432\tDev FI: 0.756\n[72,     1] loss: 0.503\tDev FI: 0.756\n[72,     2] loss: 0.431\tDev FI: 0.756\n[73,     1] loss: 0.503\tDev FI: 0.756\n[73,     2] loss: 0.430\tDev FI: 0.756\n[74,     1] loss: 0.502\tDev FI: 0.756\n[74,     2] loss: 0.429\tDev FI: 0.756\n[75,     1] loss: 0.502\tDev FI: 0.755\n[75,     2] loss: 0.428\tDev FI: 0.755\n[76,     1] loss: 0.502\tDev FI: 0.755\n[76,     2] loss: 0.427\tDev FI: 0.753\n[77,     1] loss: 0.502\tDev FI: 0.753\n[77,     2] loss: 0.426\tDev FI: 0.754\n[78,     1] loss: 0.502\tDev FI: 0.754\n[78,     2] loss: 0.425\tDev FI: 0.754\n[79,     1] loss: 0.501\tDev FI: 0.754\n[79,     2] loss: 0.424\tDev FI: 0.755\n[80,     1] loss: 0.501\tDev FI: 0.756\n[80,     2] loss: 0.423\tDev FI: 0.755\n[81,     1] loss: 0.501\tDev FI: 0.755\n[81,     2] loss: 0.422\tDev FI: 0.755\n[82,     1] loss: 0.501\tDev FI: 0.755\n[82,     2] loss: 0.421\tDev FI: 0.755\n[83,     1] loss: 0.501\tDev FI: 0.756\n[83,     2] loss: 0.420\tDev FI: 0.755\n[84,     1] loss: 0.500\tDev FI: 0.755\n[84,     2] loss: 0.420\tDev FI: 0.755\n[85,     1] loss: 0.500\tDev FI: 0.754\n[85,     2] loss: 0.419\tDev FI: 0.754\n[86,     1] loss: 0.500\tDev FI: 0.754\n[86,     2] loss: 0.418\tDev FI: 0.755\n[87,     1] loss: 0.500\tDev FI: 0.755\n[87,     2] loss: 0.417\tDev FI: 0.755\n[88,     1] loss: 0.500\tDev FI: 0.754\n[88,     2] loss: 0.416\tDev FI: 0.754\n[89,     1] loss: 0.500\tDev FI: 0.754\n[89,     2] loss: 0.415\tDev FI: 0.754\n[90,     1] loss: 0.499\tDev FI: 0.755\n[90,     2] loss: 0.414\tDev FI: 0.754\n[91,     1] loss: 0.499\tDev FI: 0.754\n[91,     2] loss: 0.413\tDev FI: 0.753\n[92,     1] loss: 0.499\tDev FI: 0.753\n[92,     2] loss: 0.412\tDev FI: 0.753\n[93,     1] loss: 0.499\tDev FI: 0.754\n[93,     2] loss: 0.411\tDev FI: 0.754\n[94,     1] loss: 0.499\tDev FI: 0.754\n[94,     2] loss: 0.410\tDev FI: 0.754\n[95,     1] loss: 0.499\tDev FI: 0.755\n[95,     2] loss: 0.409\tDev FI: 0.755\n[96,     1] loss: 0.499\tDev FI: 0.755\n[96,     2] loss: 0.408\tDev FI: 0.755\n[97,     1] loss: 0.498\tDev FI: 0.755\n[97,     2] loss: 0.407\tDev FI: 0.754\n[98,     1] loss: 0.498\tDev FI: 0.754\n[98,     2] loss: 0.407\tDev FI: 0.753\n[99,     1] loss: 0.498\tDev FI: 0.753\n[99,     2] loss: 0.406\tDev FI: 0.753\n[100,     1] loss: 0.498\tDev FI: 0.753\n[100,     2] loss: 0.405\tDev FI: 0.753\n"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(dan)\n",
    "net = dan.Net(hiddenDim = 2)\n",
    "net.train(train, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Performance and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dev F1: 0.7530536378120021\n"
    }
   ],
   "source": [
    "# dev[0]\n",
    "ys, y_stars = net.get_eval_data(dev)\n",
    "# confmat(ys, y_stars)\n",
    "print('Dev F1: {}'.format(f1(ys, y_stars)))\n",
    "\n",
    "twt['pred'] = pd.Series(y_stars, index = dev_idx)\n",
    "\n",
    "\n",
    "# for error analysis\n",
    "twt[['keyword', 'location', 'text', 'prepped','vec', 'target','pred']].to_csv('error_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3263 tweets read from test.csv\n3263 tweets processed in the test set.\n"
    }
   ],
   "source": [
    "# create vector of mean of all word vectors\n",
    "mean_vec = np.zeros((300,1))\n",
    "for vec_targ in train:\n",
    "    mean_vec = np.add(mean_vec, vec_targ[0])\n",
    "mean_vec = mean_vec/len(train)\n",
    "\n",
    "# read in test data and preprocess tweets\n",
    "test = pd.read_csv('test.csv')\n",
    "text = test['text']\n",
    "proc_text = [preprocess.processing(i) for i in text]\n",
    "targets = np.zeros((len(test), 1))\n",
    "print('{} tweets read from test.csv'.format(test.shape[0]))\n",
    "\n",
    "\n",
    "# test_data = [(tweet_vec(x, word2vec), y) if for x, y in zip(proc_test, targets)]\n",
    "test['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in proc_text], index = test.index)\n",
    "\n",
    "print('{} tweets processed in the test set.'.format(test.shape[0]))\n",
    "\n",
    "test_data = test['vec'].tolist()\n",
    "\n",
    "\n",
    "_, y_stars = net.get_eval_data(test_data, mode = 'test')\n",
    "\n",
    "# create columns for submission data\n",
    "id = test['id'].to_numpy()\n",
    "target = y_stars\n",
    "\n",
    "# create df with submission data and write to csv\n",
    "submission = pd.DataFrame({'id': id, 'target': target})\n",
    "submission.set_index('id').to_csv('submission.csv')\n",
    "\n",
    "# also save analysis csv for reviewing decisions\n",
    "test['prepped'] = proc_text\n",
    "test['pred'] = target\n",
    "test[['keyword', 'location', 'text', 'prepped','pred']].to_csv('test_error_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id', 'pred']].set_index('id').to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Created 7613 bag-of-words vectorizations.\nNaive Bayes training complete.\nNaive Bayes dev performance 0.7500000000000001\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create raw bow text and labels, shuffle\n",
    "bow = twt['text'].to_numpy()\n",
    "labels = twt['target'].to_numpy()\n",
    "\n",
    "shuffle_idx = [x for x in range(len(bow))]\n",
    "random.shuffle(shuffle_idx)\n",
    "\n",
    "bow = bow[shuffle_idx]\n",
    "labels = labels[shuffle_idx]\n",
    "\n",
    "# train bag-of-words\n",
    "count_vect = CountVectorizer()\n",
    "x_bow = count_vect.fit(bow)\n",
    "\n",
    "# create train dev split\n",
    "train_bow = bow[:round(train_p*len(bow))]\n",
    "dev_bow = bow[round(train_p*len(bow)):]\n",
    "train_bow = count_vect.transform(train_bow)\n",
    "dev_bow = count_vect.transform(dev_bow)\n",
    "\n",
    "train_bow_labels = labels[:round(train_p*len(bow))]\n",
    "dev_bow_lables = labels[round(train_p*len(bow)):]\n",
    "\n",
    "print('Created {} bag-of-words vectorizations.'.format(train_bow.shape[0] + dev_bow.shape[0]))\n",
    "clf = MultinomialNB().fit(train_bow, train_bow_labels)\n",
    "print('Naive Bayes training complete.')\n",
    "\n",
    "# get dev performances\n",
    "ys = clf.predict(dev_bow)\n",
    "\n",
    "# dev performance\n",
    "print('Naive Bayes dev performance {}'.format(f1(ys, dev_bow_lables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness Exploration\n",
    "\n",
    "We suspect that location data can be useful for predicting disasters. However, we have a sizable proportion of the data for which the location data is missing. In order to address this issue, we needed to first identify whether the data is missing completely at random, missing at random, or not missing at random. For the data to be MCAR, the mechanism of missingness of the data must be independent of all other observed features of the data. These other observed features, in our case, include the text of the tweet. Reviewing the Twitter policy on tweet-level location data, we found that twitter users must opt in to location services to embed location data in there tweets, as well as intentionally include location data on each desired tweet. This policy gives us an insight into some mechanisms of missingness - users forget that they can include location data, or the intentionally elect not to for some tweets. This second mechanism would preclude categorization as missing completely at random. Furthermore, as users of social media, the authors have firsthand experience with the relationship between the location from which a tweet was sent, and the inclusion of location data. Twitter users may want to inform followers that they are tweeting from an impressive or otherwise unusual location - a famous concert venue, or a historic city, or even the site of a terrible natural disaster - a decision surely not made at random. Being Missing Not a Random, our options for mitigating location data missingness are few. We could drop the data altogether, but decided that appending it to the contents of the tweet would be an acceptable approach that prevents information loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Finished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\nFinished training\n"
    }
   ],
   "source": [
    "# hidden Layer dimensions\n",
    "pre_opts = [(1, 0, 0), (1, 1, 0), (1, 0, 1) , (1, 1, 1), (0, 1, 0), (0, 1, 1), (0, 0, 1)]\n",
    "Fs_ls = []\n",
    "for column in ['text', 'text+', 'text++']:\n",
    "    text = twt[column]\n",
    "    for lower, stopwords, stem in pre_opts:\n",
    "        prep_text = [preprocess.processing(i, lower, stopwords, stem) for i in text]\n",
    "        # print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "\n",
    "        twt['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in prep_text], index = twt.index)\n",
    "\n",
    "        devtrain_idx = twt.loc[~twt['vec'].isna()].index.tolist()\n",
    "        random.shuffle(devtrain_idx)\n",
    "\n",
    "        train_p = 0.7\n",
    "        train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "        dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "        train = [(vec, targ) for targ, vec in  zip(twt['target'][train_idx], twt['vec'][train_idx])]\n",
    "        dev = [(vec, targ) for targ, vec in  zip(twt['target'][dev_idx], twt['vec'][dev_idx])]\n",
    "\n",
    "        Fs = []\n",
    "        for hiddenDim in range(2,150,10):\n",
    "            # print('Executing model with hidden layer dimensionality {}'.format(hiddenDim))\n",
    "            net = dan.Net(hiddenDim)\n",
    "            net.train(train, dev, verbose = False)\n",
    "            ys, y_stars = net.get_eval_da\n",
    "            ta(dev)\n",
    "            Fs.append(f1(ys, y_stars))\n",
    "            print('Finished training')\n",
    "        Fs_ls.append(Fs)\n",
    "\n",
    "results = {i : Fs_ls[i] for i in range(len(Fs_ls))}\n",
    "pd.DataFrame(results).to_csv('hiddenDim_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a grid search dev evaluation of the hiddenDim and preprocessing hyperparameters, we found that hiddenDim = 52, lowercasing, and stopword inclusion (1, 1, 0) provided the best results, at F1 = 0.744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size and Class Balance of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Finished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\nFinished Training.\n"
    }
   ],
   "source": [
    "samples = [pd.concat([twt, add.sample(n = N)], axis = 0) for N in [0, 2000, 5000, 10000, 15000]]\n",
    "\n",
    "Fs_ls  = []\n",
    "FPR_ls = []\n",
    "for sample in samples:\n",
    "    text = sample['text']\n",
    "    prep_text = [preprocess.processing(i) for i in text]\n",
    "    # print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "\n",
    "    sample['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in prep_text], index = sample.index)\n",
    "\n",
    "    devtrain_idx = sample.loc[~sample['vec'].isna()].index.tolist()\n",
    "    random.shuffle(devtrain_idx)\n",
    "\n",
    "    train_p = 0.7\n",
    "    train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "    dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "    train = [(vec, targ) for targ, vec in  zip(sample['target'][train_idx], sample['vec'][train_idx])]\n",
    "    dev = [(vec, targ) for targ, vec in  zip(sample['target'][dev_idx], sample['vec'][dev_idx])]\n",
    "\n",
    "    Fs = []\n",
    "    FPR = []\n",
    "    for hiddenDim in range(2,302,25):\n",
    "        # print('Executing model with hidden layer dimensionality {}'.format(hiddenDim))\n",
    "        net = dan.Net(hiddenDim)\n",
    "        net.train(train, dev, verbose = False)\n",
    "        ys, y_stars = net.get_eval_data(dev)\n",
    "        Fs.append(f1(ys, y_stars))\n",
    "        # false positive rate\n",
    "        FPR.append(np.sum([1 if (y == 0) and (y_star == 1) else 0 for y, y_star in zip(ys, y_stars)])/(len(ys) - np.sum(ys)))\n",
    "    Fs_ls.append(Fs)\n",
    "    FPR_ls.append(FPR)\n",
    "\n",
    "f1_results = {i : Fs_ls[i] for i in range(len(Fs_ls))}\n",
    "fpr_results = {i : FPR_ls[i] for i in range(len(FPR_ls))}\n",
    "pd.DataFrame(fpr_results).to_csv('data_dim_ablation_fpr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate, Epochs and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dan)\n",
    "Fs_ls  = []\n",
    "text = twt['text']\n",
    "prep_text = [preprocess.processing(i) for i in text]\n",
    "# print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "\n",
    "twt['vec'] = pd.Series([preprocess.tweet_vec(tweet, word2vec) for tweet in prep_text], index = twt.index)\n",
    "\n",
    "devtrain_idx = twt.loc[~twt['vec'].isna()].index.tolist()\n",
    "random.shuffle(devtrain_idx)\n",
    "\n",
    "train_p = 0.7\n",
    "train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "train = [(vec, targ) for targ, vec in  zip(sample['target'][train_idx], sample['vec'][train_idx])]\n",
    "dev = [(vec, targ) for targ, vec in  zip(sample['target'][dev_idx], sample['vec'][dev_idx])]\n",
    "\n",
    "Fs = []\n",
    "for opter in ['adam', 'sgd']:\n",
    "    # print('Executing model with hidden layer dimensionality {}'.format(hiddenDim))\n",
    "    for epochs in [5, 10, 50, 100]:\n",
    "        Fs = []\n",
    "        for lr in np.logspace(-4, -1, 7):\n",
    "            net = dan.Net(hiddenDim)\n",
    "            net.train(train, dev, verbose = False, opter = opter, lr = lr, epochs = epochs)\n",
    "            ys, y_stars = net.get_eval_data(dev)\n",
    "            Fs.append(f1(ys, y_stars))\n",
    "            # false positive rate\n",
    "        Fs_ls.append(Fs)\n",
    "\n",
    "f1_results = {i : Fs_ls[i] for i in range(len(Fs_ls))}\n",
    "pd.DataFrame(f1_results).to_csv('epochs_lr_opter_ablation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}