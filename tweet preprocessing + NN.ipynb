{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix as confmat\n",
    "# from sklearn.metrics import plot_confusion_matrix as plot_confmat\n",
    "\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset and preprocess\n",
    "twt = pd.read_csv('train.csv')\n",
    "twt = twt.set_index('id')\n",
    "twt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7613 entries, 1 to 10873\n",
      "Data columns (total 4 columns):\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 297.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                       location  \\\n",
       "id                                              \n",
       "48      ablaze                     Birmingham   \n",
       "49      ablaze  Est. September 2012 - Bristol   \n",
       "50      ablaze                         AFRICA   \n",
       "52      ablaze               Philadelphia, PA   \n",
       "53      ablaze                     London, UK   \n",
       "...        ...                            ...   \n",
       "10830  wrecked                            NaN   \n",
       "10831  wrecked              Vancouver, Canada   \n",
       "10832  wrecked                        London    \n",
       "10833  wrecked                        Lincoln   \n",
       "10834  wrecked                            NaN   \n",
       "\n",
       "                                                    text  target  \n",
       "id                                                                \n",
       "48     @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "49     We always try to bring the heavy. #metal #RT h...       0  \n",
       "50     #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "52                    Crying out for more! Set me ablaze       0  \n",
       "53     On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n",
       "...                                                  ...     ...  \n",
       "10830   @jt_ruff23 @cameronhacker and I wrecked you both       0  \n",
       "10831  Three days off from work and they've pretty mu...       0  \n",
       "10832  #FX #forex #trading Cramer: Iger's 3 words tha...       0  \n",
       "10833  @engineshed Great atmosphere at the British Li...       0  \n",
       "10834  Cramer: Iger's 3 words that wrecked Disney's s...       0  \n",
       "\n",
       "[7552 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(twt.info())\n",
    "twt.loc[~twt['keyword'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing (sentence):\n",
    "    result = sentence.lower() #Lower case \n",
    "    result = re.sub(r'\\d+', '', result) #Removing numbers\n",
    "    result = result.translate(str.maketrans('', '', string.punctuation)) #Remove weird characters\n",
    "    result = result.strip() #Eliminate blanks from begining and end of setences\n",
    "    result = result.split() #Separate into words\n",
    "    result = [w for w in result if not w in stop_words] #Eliminate stop_words\n",
    "    result = [porter.stem(word) for word in result] #Stem Words\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = list(twt[\"text\"])\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_vec (tweet, word2vec):\n",
    "    word_vecs = [word2vec.get_vector(w) for w in tweet if w in word2vec.vocab]\n",
    "#     print(tweet)\n",
    "#     print('Number of words: {}'.format(len(word_vecs)))\n",
    "    if len(word_vecs) >= 1:\n",
    "        return np.stack(word_vecs).mean(0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\n"
     ]
    }
   ],
   "source": [
    "prep_text = [processing(i) for i in text]\n",
    "print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n",
      "5311 tweets vectorized for the training set.\n",
      "2276 tweets vectorized for the dev set.\n",
      "26 tweets were found to be empty.\n"
     ]
    }
   ],
   "source": [
    "final_data = []\n",
    "targets = twt['target'].to_numpy()\n",
    "\n",
    "# prep_text[0]\n",
    "# tweet_vec(prep_text[0], word2vec)\n",
    "'''\n",
    "for i in prep_text:\n",
    "    vec = tweet_vec(i,word2vec)\n",
    "    if vec is not None:   \n",
    "        final_data.append(vec)\n",
    "'''      \n",
    "number_empty = 0\n",
    "for x, y in zip(prep_text, targets):\n",
    "    vec = tweet_vec(x, word2vec)\n",
    "    if vec is not None:\n",
    "        final_data.append((vec, y))\n",
    "    else:\n",
    "        number_empty += 1\n",
    "        \n",
    "train_p = 0.70\n",
    "random.shuffle(final_data)\n",
    "\n",
    "# train is final data \n",
    "train = final_data[:round(train_p*len(final_data))]\n",
    "dev = final_data[round(train_p*len(final_data)):]\n",
    "print('Preprocessing complete.\\n{} tweets vectorized for the training set.'.format(len(train)))\n",
    "print('{} tweets vectorized for the dev set.\\n{} tweets were found to be empty.'.format(len(dev), number_empty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.693\n",
      "[1,     2] loss: 0.647\n",
      "[1,     3] loss: 0.620\n",
      "[1,     4] loss: 0.585\n",
      "[1,     5] loss: 0.560\n",
      "[1,     6] loss: 0.541\n",
      "[2,     1] loss: 0.545\n",
      "[2,     2] loss: 0.556\n",
      "[2,     3] loss: 0.544\n",
      "[2,     4] loss: 0.532\n",
      "[2,     5] loss: 0.523\n",
      "[2,     6] loss: 0.516\n",
      "[3,     1] loss: 0.534\n",
      "[3,     2] loss: 0.543\n",
      "[3,     3] loss: 0.539\n",
      "[3,     4] loss: 0.523\n",
      "[3,     5] loss: 0.521\n",
      "[3,     6] loss: 0.488\n",
      "[4,     1] loss: 0.531\n",
      "[4,     2] loss: 0.526\n",
      "[4,     3] loss: 0.525\n",
      "[4,     4] loss: 0.514\n",
      "[4,     5] loss: 0.506\n",
      "[4,     6] loss: 0.480\n",
      "[5,     1] loss: 0.526\n",
      "[5,     2] loss: 0.528\n",
      "[5,     3] loss: 0.523\n",
      "[5,     4] loss: 0.510\n",
      "[5,     5] loss: 0.498\n",
      "[5,     6] loss: 0.473\n",
      "[6,     1] loss: 0.527\n",
      "[6,     2] loss: 0.522\n",
      "[6,     3] loss: 0.514\n",
      "[6,     4] loss: 0.504\n",
      "[6,     5] loss: 0.496\n",
      "[6,     6] loss: 0.457\n",
      "[7,     1] loss: 0.521\n",
      "[7,     2] loss: 0.519\n",
      "[7,     3] loss: 0.511\n",
      "[7,     4] loss: 0.499\n",
      "[7,     5] loss: 0.489\n",
      "[7,     6] loss: 0.452\n",
      "[8,     1] loss: 0.513\n",
      "[8,     2] loss: 0.517\n",
      "[8,     3] loss: 0.506\n",
      "[8,     4] loss: 0.496\n",
      "[8,     5] loss: 0.485\n",
      "[8,     6] loss: 0.454\n",
      "[9,     1] loss: 0.511\n",
      "[9,     2] loss: 0.521\n",
      "[9,     3] loss: 0.506\n",
      "[9,     4] loss: 0.495\n",
      "[9,     5] loss: 0.485\n",
      "[9,     6] loss: 0.456\n",
      "[10,     1] loss: 0.509\n",
      "[10,     2] loss: 0.517\n",
      "[10,     3] loss: 0.510\n",
      "[10,     4] loss: 0.496\n",
      "[10,     5] loss: 0.483\n",
      "[10,     6] loss: 0.457\n",
      "Finished Training\n",
      "<generator object Module.parameters at 0x0000010798FB40C8>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(300,300)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(300, 2)\n",
    "        self.softmax =  nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size = 1000)\n",
    "net = Net()\n",
    "\n",
    "# print(net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         print(inputs)\n",
    "#         print(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1 == 0: # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "print(net.parameters())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1117,  196],\n",
       "       [ 315,  648]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devloader = torch.utils.data.DataLoader(dev, batch_size = 1)\n",
    "\n",
    "y_stars = []\n",
    "ys = [vec_targ[1] for vec_targ in dev]\n",
    "\n",
    "for i, data in enumerate(devloader, 0):\n",
    "    x, y = data\n",
    "    # print(x)\n",
    "    output = net.forward(x).detach().numpy()[0]\n",
    "    y_star = np.argmax(output)\n",
    "    #print(y_star)\n",
    "    y_stars.append(y_star)\n",
    "\n",
    "confmat(ys, y_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263 tweets read from test.csv\n",
      "3263 tweets processed in the test set.\n",
      "0 empty tweets replaced with mean vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# create vector of mean of all word vectors\n",
    "mean_vec = np.zeros((300,1))\n",
    "for vec_targ in train:\n",
    "    mean_vec = np.add(mean_vec, vec_targ[0])\n",
    "mean_vec = mean_vec/len(final_data)\n",
    "\n",
    "# read in test data and preprocess tweets\n",
    "test = pd.read_csv('test.csv')\n",
    "text = list(test[\"text\"])\n",
    "proc_text = [processing(i) for i in text]\n",
    "targets = np.zeros((len(test), 1))\n",
    "print('{} tweets read from test.csv'.format(test.shape[0]))\n",
    "\n",
    "test_data = []\n",
    "counter = 0\n",
    "# replace empty vectors with mean_vec\n",
    "for x, y in zip(prep_text, targets):\n",
    "    vec = tweet_vec(x, word2vec)\n",
    "    if vec is not None:\n",
    "        test_data.append((vec, y))\n",
    "    else:\n",
    "        test_data.append((mean_vec, y))\n",
    "        counter += 0\n",
    "        \n",
    "print('{} tweets processed in the test set.'.format(len(test_data)))\n",
    "print('{} empty tweets replaced with mean vector'.format(counter))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 1)\n",
    "\n",
    "y_stars = []\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    x, y = data\n",
    "    #print(x.dtype)\n",
    "    #x_dub = torch.Tensor(x, dtype = 'double')\n",
    "    #print(x.dtype)\n",
    "    output = net.forward(x.float()).detach().numpy()[0]\n",
    "    y_star = np.argmax(output)\n",
    "    #print(y_star)\n",
    "    y_stars.append(y_star)\n",
    "\n",
    "# create columns for submission data\n",
    "id = test['id'].to_numpy()\n",
    "target = y_stars\n",
    "\n",
    "# create df with submission data and write to csv\n",
    "submission = pd.DataFrame({'id': id, 'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.set_index('id')\n",
    "submission.to_csv('submission_3_19.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
