{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   keyword location                                               text  target\nid                                                                            \n1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix as confmat\n",
    "from sklearn.metrics import f1_score as f1\n",
    "# from sklearn.metrics import plot_confusion_matrix as plot_confmat\n",
    "\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "# load dataset and preprocess\n",
    "twt = pd.read_csv('train.csv')\n",
    "twt = twt.set_index('id')\n",
    "twt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\n"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "def processing (sentence):\n",
    "    result = sentence.lower() #Lower case \n",
    "    result = re.sub(r'\\d+', '', result) #Removing numbers\n",
    "    result = result.translate(str.maketrans('', '', string.punctuation)) #Remove weird characters\n",
    "    result = result.strip() #Eliminate blanks from begining and end of setences\n",
    "    result = result.split() #Separate into words\n",
    "    result = [w for w in result if not w in stop_words] #Eliminate stop_words\n",
    "    # result = [porter.stem(word) for word in result] #Stem Words\n",
    "    return (result)\n",
    "\n",
    "text = list(twt[\"text\"])\n",
    "len(text)\n",
    "\n",
    "prep_text = [processing(i) for i in text]\n",
    "print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "twt['prepped'] = prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(7613, 5)\n(7613, 5)\n"
    }
   ],
   "source": [
    "# twt.loc[twt['keyword'].any()),]\n",
    "print(twt.shape)\n",
    "twt.dropna()\n",
    "print(twt.shape)\n",
    "# twt['prepped'].isna().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "def tweet_vec (tweet, word2vec):\n",
    "    word_vecs = [word2vec.get_vector(w) for w in tweet if w in word2vec.vocab]\n",
    "#     print(tweet)\n",
    "#     print('Number of words: {}'.format(len(word_vecs)))\n",
    "    if len(word_vecs) >= 1:\n",
    "        return np.stack(word_vecs).mean(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "twt['vec'] = pd.Series([tweet_vec(tweet, word2vec) for tweet in twt['prepped']], index = twt.index)\n",
    "\n",
    "devtrain_idx = twt.loc[~twt['vec'].isna()].index.tolist()\n",
    "random.shuffle(devtrain_idx)\n",
    "\n",
    "train_p = 0.7\n",
    "train_idx = devtrain_idx[:round(train_p*len(devtrain_idx))]\n",
    "dev_idx = devtrain_idx[round(train_p*len(devtrain_idx)):]\n",
    "\n",
    "train = [(vec, targ) for targ, vec in  zip(twt['target'][train_idx], twt['vec'][train_idx])]\n",
    "dev = [(vec, targ) for targ, vec in  zip(twt['target'][dev_idx], twt['vec'][dev_idx])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1,     1] loss: 0.691\tDev FI: 0.002\n[1,     2] loss: 0.678\tDev FI: 0.599\n[2,     1] loss: 0.612\tDev FI: 0.703\n[2,     2] loss: 0.584\tDev FI: 0.708\n[3,     1] loss: 0.549\tDev FI: 0.696\n[3,     2] loss: 0.535\tDev FI: 0.693\n[4,     1] loss: 0.524\tDev FI: 0.700\n[4,     2] loss: 0.516\tDev FI: 0.707\n[5,     1] loss: 0.515\tDev FI: 0.714\n[5,     2] loss: 0.503\tDev FI: 0.716\n[6,     1] loss: 0.511\tDev FI: 0.714\n[6,     2] loss: 0.489\tDev FI: 0.710\n[7,     1] loss: 0.508\tDev FI: 0.711\n[7,     2] loss: 0.474\tDev FI: 0.713\n[8,     1] loss: 0.507\tDev FI: 0.716\n[8,     2] loss: 0.462\tDev FI: 0.723\n[9,     1] loss: 0.504\tDev FI: 0.727\n[9,     2] loss: 0.451\tDev FI: 0.728\n[10,     1] loss: 0.504\tDev FI: 0.722\n[10,     2] loss: 0.443\tDev FI: 0.724\n[11,     1] loss: 0.504\tDev FI: 0.725\n[11,     2] loss: 0.437\tDev FI: 0.726\n[12,     1] loss: 0.503\tDev FI: 0.729\n[12,     2] loss: 0.429\tDev FI: 0.728\n[13,     1] loss: 0.503\tDev FI: 0.729\n[13,     2] loss: 0.423\tDev FI: 0.732\n[14,     1] loss: 0.501\tDev FI: 0.731\n[14,     2] loss: 0.420\tDev FI: 0.732\n[15,     1] loss: 0.500\tDev FI: 0.726\n[15,     2] loss: 0.418\tDev FI: 0.730\n[16,     1] loss: 0.499\tDev FI: 0.733\n[16,     2] loss: 0.416\tDev FI: 0.734\n[17,     1] loss: 0.498\tDev FI: 0.734\n[17,     2] loss: 0.414\tDev FI: 0.737\n[18,     1] loss: 0.497\tDev FI: 0.737\n[18,     2] loss: 0.411\tDev FI: 0.737\n[19,     1] loss: 0.496\tDev FI: 0.736\n[19,     2] loss: 0.407\tDev FI: 0.737\n[20,     1] loss: 0.495\tDev FI: 0.737\n[20,     2] loss: 0.406\tDev FI: 0.735\n[21,     1] loss: 0.494\tDev FI: 0.735\n[21,     2] loss: 0.404\tDev FI: 0.733\n[22,     1] loss: 0.493\tDev FI: 0.735\n[22,     2] loss: 0.402\tDev FI: 0.733\n[23,     1] loss: 0.492\tDev FI: 0.737\n[23,     2] loss: 0.399\tDev FI: 0.740\n[24,     1] loss: 0.491\tDev FI: 0.740\n[24,     2] loss: 0.399\tDev FI: 0.742\n[25,     1] loss: 0.490\tDev FI: 0.741\n[25,     2] loss: 0.397\tDev FI: 0.743\n[26,     1] loss: 0.489\tDev FI: 0.739\n[26,     2] loss: 0.396\tDev FI: 0.740\n[27,     1] loss: 0.488\tDev FI: 0.740\n[27,     2] loss: 0.396\tDev FI: 0.740\n[28,     1] loss: 0.487\tDev FI: 0.735\n[28,     2] loss: 0.396\tDev FI: 0.735\n[29,     1] loss: 0.486\tDev FI: 0.735\n[29,     2] loss: 0.395\tDev FI: 0.736\n[30,     1] loss: 0.485\tDev FI: 0.733\n[30,     2] loss: 0.395\tDev FI: 0.734\n[31,     1] loss: 0.484\tDev FI: 0.734\n[31,     2] loss: 0.395\tDev FI: 0.735\n[32,     1] loss: 0.483\tDev FI: 0.736\n[32,     2] loss: 0.395\tDev FI: 0.736\n[33,     1] loss: 0.482\tDev FI: 0.736\n[33,     2] loss: 0.395\tDev FI: 0.736\n[34,     1] loss: 0.481\tDev FI: 0.734\n[34,     2] loss: 0.395\tDev FI: 0.736\n[35,     1] loss: 0.480\tDev FI: 0.735\n[35,     2] loss: 0.395\tDev FI: 0.734\n[36,     1] loss: 0.480\tDev FI: 0.735\n[36,     2] loss: 0.395\tDev FI: 0.734\n[37,     1] loss: 0.479\tDev FI: 0.735\n[37,     2] loss: 0.395\tDev FI: 0.736\n[38,     1] loss: 0.478\tDev FI: 0.734\n[38,     2] loss: 0.395\tDev FI: 0.734\n[39,     1] loss: 0.478\tDev FI: 0.734\n[39,     2] loss: 0.395\tDev FI: 0.733\n[40,     1] loss: 0.477\tDev FI: 0.733\n[40,     2] loss: 0.395\tDev FI: 0.733\n[41,     1] loss: 0.477\tDev FI: 0.734\n[41,     2] loss: 0.394\tDev FI: 0.733\n[42,     1] loss: 0.476\tDev FI: 0.733\n[42,     2] loss: 0.395\tDev FI: 0.734\n[43,     1] loss: 0.476\tDev FI: 0.733\n[43,     2] loss: 0.395\tDev FI: 0.733\n[44,     1] loss: 0.475\tDev FI: 0.734\n[44,     2] loss: 0.395\tDev FI: 0.735\n[45,     1] loss: 0.475\tDev FI: 0.735\n[45,     2] loss: 0.394\tDev FI: 0.733\n[46,     1] loss: 0.475\tDev FI: 0.732\n[46,     2] loss: 0.395\tDev FI: 0.732\n[47,     1] loss: 0.478\tDev FI: 0.736\n[47,     2] loss: 0.395\tDev FI: 0.736\n[48,     1] loss: 0.490\tDev FI: 0.730\n[48,     2] loss: 0.395\tDev FI: 0.716\n[49,     1] loss: 0.496\tDev FI: 0.720\n[49,     2] loss: 0.403\tDev FI: 0.737\n[50,     1] loss: 0.475\tDev FI: 0.738\n[50,     2] loss: 0.404\tDev FI: 0.737\n[51,     1] loss: 0.477\tDev FI: 0.724\n[51,     2] loss: 0.395\tDev FI: 0.717\n[52,     1] loss: 0.499\tDev FI: 0.719\n[52,     2] loss: 0.402\tDev FI: 0.732\n[53,     1] loss: 0.477\tDev FI: 0.736\n[53,     2] loss: 0.396\tDev FI: 0.733\n[54,     1] loss: 0.518\tDev FI: 0.737\n[54,     2] loss: 0.396\tDev FI: 0.738\n[55,     1] loss: 0.476\tDev FI: 0.723\n[55,     2] loss: 0.395\tDev FI: 0.714\n[56,     1] loss: 0.502\tDev FI: 0.718\n[56,     2] loss: 0.405\tDev FI: 0.729\n[57,     1] loss: 0.480\tDev FI: 0.736\n[57,     2] loss: 0.392\tDev FI: 0.738\n[58,     1] loss: 0.518\tDev FI: 0.739\n[58,     2] loss: 0.426\tDev FI: 0.735\n[59,     1] loss: 0.479\tDev FI: 0.713\n[59,     2] loss: 0.403\tDev FI: 0.695\n[60,     1] loss: 0.520\tDev FI: 0.689\n[60,     2] loss: 0.461\tDev FI: 0.711\n[61,     1] loss: 0.503\tDev FI: 0.728\n[61,     2] loss: 0.392\tDev FI: 0.731\n[62,     1] loss: 0.497\tDev FI: 0.733\n[62,     2] loss: 0.399\tDev FI: 0.733\n[63,     1] loss: 0.516\tDev FI: 0.732\n[63,     2] loss: 0.392\tDev FI: 0.735\n[64,     1] loss: 0.490\tDev FI: 0.729\n[64,     2] loss: 0.391\tDev FI: 0.725\n[65,     1] loss: 0.490\tDev FI: 0.720\n[65,     2] loss: 0.394\tDev FI: 0.722\n[66,     1] loss: 0.499\tDev FI: 0.722\n[66,     2] loss: 0.395\tDev FI: 0.722\n[67,     1] loss: 0.492\tDev FI: 0.726\n[67,     2] loss: 0.390\tDev FI: 0.730\n[68,     1] loss: 0.484\tDev FI: 0.739\n[68,     2] loss: 0.390\tDev FI: 0.742\n[69,     1] loss: 0.488\tDev FI: 0.742\n[69,     2] loss: 0.392\tDev FI: 0.742\n[70,     1] loss: 0.489\tDev FI: 0.741\n[70,     2] loss: 0.391\tDev FI: 0.737\n[71,     1] loss: 0.483\tDev FI: 0.729\n[71,     2] loss: 0.390\tDev FI: 0.726\n[72,     1] loss: 0.482\tDev FI: 0.726\n[72,     2] loss: 0.391\tDev FI: 0.727\n[73,     1] loss: 0.484\tDev FI: 0.726\n[73,     2] loss: 0.390\tDev FI: 0.730\n[74,     1] loss: 0.481\tDev FI: 0.733\n[74,     2] loss: 0.390\tDev FI: 0.736\n[75,     1] loss: 0.479\tDev FI: 0.735\n[75,     2] loss: 0.390\tDev FI: 0.738\n[76,     1] loss: 0.480\tDev FI: 0.735\n[76,     2] loss: 0.390\tDev FI: 0.736\n[77,     1] loss: 0.478\tDev FI: 0.732\n[77,     2] loss: 0.390\tDev FI: 0.732\n[78,     1] loss: 0.477\tDev FI: 0.732\n[78,     2] loss: 0.390\tDev FI: 0.731\n[79,     1] loss: 0.476\tDev FI: 0.731\n[79,     2] loss: 0.390\tDev FI: 0.730\n[80,     1] loss: 0.474\tDev FI: 0.734\n[80,     2] loss: 0.390\tDev FI: 0.738\n[81,     1] loss: 0.474\tDev FI: 0.734\n[81,     2] loss: 0.390\tDev FI: 0.731\n[82,     1] loss: 0.473\tDev FI: 0.733\n[82,     2] loss: 0.390\tDev FI: 0.731\n[83,     1] loss: 0.473\tDev FI: 0.733\n[83,     2] loss: 0.390\tDev FI: 0.731\n[84,     1] loss: 0.472\tDev FI: 0.735\n[84,     2] loss: 0.390\tDev FI: 0.734\n[85,     1] loss: 0.472\tDev FI: 0.735\n[85,     2] loss: 0.390\tDev FI: 0.735\n[86,     1] loss: 0.471\tDev FI: 0.733\n[86,     2] loss: 0.390\tDev FI: 0.733\n[87,     1] loss: 0.471\tDev FI: 0.732\n[87,     2] loss: 0.390\tDev FI: 0.732\n[88,     1] loss: 0.471\tDev FI: 0.735\n[88,     2] loss: 0.390\tDev FI: 0.736\n[89,     1] loss: 0.470\tDev FI: 0.735\n[89,     2] loss: 0.390\tDev FI: 0.734\n[90,     1] loss: 0.470\tDev FI: 0.734\n[90,     2] loss: 0.390\tDev FI: 0.732\n[91,     1] loss: 0.470\tDev FI: 0.733\n[91,     2] loss: 0.390\tDev FI: 0.734\n[92,     1] loss: 0.470\tDev FI: 0.733\n[92,     2] loss: 0.390\tDev FI: 0.732\n[93,     1] loss: 0.469\tDev FI: 0.734\n[93,     2] loss: 0.390\tDev FI: 0.735\n[94,     1] loss: 0.469\tDev FI: 0.735\n[94,     2] loss: 0.390\tDev FI: 0.735\n[95,     1] loss: 0.469\tDev FI: 0.735\n[95,     2] loss: 0.390\tDev FI: 0.734\n[96,     1] loss: 0.469\tDev FI: 0.734\n[96,     2] loss: 0.390\tDev FI: 0.734\n[97,     1] loss: 0.468\tDev FI: 0.735\n[97,     2] loss: 0.391\tDev FI: 0.737\n[98,     1] loss: 0.468\tDev FI: 0.737\n[98,     2] loss: 0.391\tDev FI: 0.737\n[99,     1] loss: 0.468\tDev FI: 0.736\n[99,     2] loss: 0.391\tDev FI: 0.736\n[100,     1] loss: 0.468\tDev FI: 0.737\n[100,     2] loss: 0.391\tDev FI: 0.737\nFinished Training\n<generator object Module.parameters at 0x0000028D440E15C8>\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(300,300)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(300, 2)\n",
    "        self.softmax =  nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def get_eval_data(self, data):\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size = 1)\n",
    "\n",
    "        y_stars = []\n",
    "        ys = [vec_targ[1] for vec_targ in data]\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            x, _ = data\n",
    "            # print(x)\n",
    "            output = self.forward(x).detach().numpy()[0]\n",
    "            y_star = np.argmax(output)\n",
    "            #print(y_star)\n",
    "            y_stars.append(y_star)\n",
    "        \n",
    "        return ys, y_stars\n",
    "\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size = 5000)\n",
    "net = Net()\n",
    "# print(net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         print(inputs)\n",
    "#         print(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1 == 0: # print every 2000 mini-batches\n",
    "            ys, y_stars = net.get_eval_data(dev)\n",
    "            print('[%d, %5d] loss: %.3f\\tDev FI: %.3f' % (epoch + 1, i + 1, running_loss, f1(ys, y_stars)))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "print(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Performance and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev[0]\n",
    "ys, y_stars = net.get_eval_data(dev)\n",
    "# confmat(ys, y_stars)\n",
    "f1(ys, y_stars)\n",
    "\n",
    "twt['pred'] = pd.Series(y_stars, index = dev_idx)\n",
    "\n",
    "\n",
    "# for error analysis\n",
    "twt[['keyword', 'location', 'text', 'prepped','vec', 'target','pred']].to_csv('error_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3263 tweets read from test.csv\n3263 tweets processed in the test set.\n0 empty tweets (0.0%) replaced with mean vector\n"
    }
   ],
   "source": [
    "# create vector of mean of all word vectors\n",
    "mean_vec = np.zeros((300,1))\n",
    "for vec_targ in train:\n",
    "    mean_vec = np.add(mean_vec, vec_targ[0])\n",
    "mean_vec = mean_vec/len(train)\n",
    "\n",
    "# read in test data and preprocess tweets\n",
    "test = pd.read_csv('test.csv')\n",
    "text = list(test[\"text\"])\n",
    "proc_text = [processing(i) for i in text]\n",
    "targets = np.zeros((len(test), 1))\n",
    "print('{} tweets read from test.csv'.format(test.shape[0]))\n",
    "\n",
    "test_data = []\n",
    "counter = 0\n",
    "\n",
    "# replace empty vectors with mean_vec\n",
    "\n",
    "# test_data = [(tweet_vec(x, word2vec), y) if for x, y in zip(proc_test, targets)]\n",
    "for x, y in zip(proc_text, targets):\n",
    "    vec = tweet_vec(x, word2vec)\n",
    "    if vec is not None:\n",
    "        test_data.append((vec, y))\n",
    "    else:\n",
    "        test_data.append((mean_vec, y))\n",
    "        counter += 0\n",
    "\n",
    "print('{} tweets processed in the test set.'.format(len(test_data)))\n",
    "print('{} empty tweets ({}%) replaced with mean vector'.format(counter, 100*counter/len(proc_text)))\n",
    "\n",
    "# print(test_data[0])\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 1)\n",
    "\n",
    "y_stars = []\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    x, y = data\n",
    "    #print(x.dtype)\n",
    "    #x_dub = torch.Tensor(x, dtype = 'double')\n",
    "    #print(x.dtype)\n",
    "    output = net.forward(x.float()).detach().numpy()[0]\n",
    "    y_star = np.argmax(output)\n",
    "    #print(y_star)\n",
    "    y_stars.append(y_star)\n",
    "\n",
    "# create columns for submission data\n",
    "id = test['id'].to_numpy()\n",
    "target = y_stars\n",
    "\n",
    "# create df with submission data and write to csv\n",
    "submission = pd.DataFrame({'id': id, 'target': target})\n",
    "submission.set_index('id').to_csv('submission.csv')\n",
    "\n",
    "# also save analysis csv for reviewing decisions\n",
    "test['prepped'] = proc_text\n",
    "test['pred'] = target\n",
    "test[['keyword', 'location', 'text', 'prepped','pred']].to_csv('test_error_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'submission' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d75e9013b4cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission_3_19.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'submission' is not defined"
     ]
    }
   ],
   "source": [
    "submission = submission.set_index('id')\n",
    "submission.to_csv('submission_3_19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_p' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a18260c6afbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# create train dev split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrain_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_p\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mdev_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_p\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mtrain_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_p' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create raw bow text and labels, shuffle\n",
    "bow = twt['text'].to_numpy()\n",
    "labels = twt['target'].to_numpy()\n",
    "\n",
    "shuffle_idx = [x for x in range(len(bow))]\n",
    "random.shuffle(shuffle_idx)\n",
    "\n",
    "bow = bow[shuffle_idx]\n",
    "labels = labels[shuffle_idx]\n",
    "\n",
    "# train bag-of-words\n",
    "count_vect = CountVectorizer()\n",
    "x_bow = count_vect.fit(bow)\n",
    "\n",
    "# create train dev split\n",
    "train_bow = bow[:round(train_p*len(bow))]\n",
    "dev_bow = bow[round(train_p*len(bow)):]\n",
    "train_bow = count_vect.transform(train_bow)\n",
    "dev_bow = count_vect.transform(dev_bow)\n",
    "\n",
    "train_bow_labels = labels[:round(train_p*len(bow))]\n",
    "dev_bow_lables = labels[round(train_p*len(bow)):]\n",
    "\n",
    "print('Created {} bag-of-words vectorizations.'.format(train_bow.shape[0] + dev_bow.shape[0]))\n",
    "clf = MultinomialNB().fit(train_bow, train_bow_labels)\n",
    "print('Naive Bayes training complete.')\n",
    "\n",
    "# get dev performances\n",
    "ys = clf.predict(dev_bow)\n",
    "\n",
    "# dev performance\n",
    "print('Naive Bayes dev performance {}'.format(f1(ys, dev_bow_lables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness Exploration\n",
    "\n",
    "We suspect that location data can be useful for predicting disasters. However, we have a sizable proportion of the data for which the location data is missing. In order to address this issue, we needed to first identify whether the data is missing completely at random, missing at random, or not missing at random. For the data to be MCAR, the mechanism of missingness of the data must be independent of all other observed features of the data. These other observed features, in our case, include the text of the tweet. Reviewing the Twitter policy on tweet-level location data, we found that twitter users must opt in to location services to embed location data in there tweets, as well as intentionally include location data on each desired tweet. This policy gives us an insight into some mechanisms of missingness - users forget that they can include location data, or the intentionally elect not to for some tweets. This second mechanism would preclude categorization as missing completely at random. Furthermore, as users of social media, we have firsthand experience with the relationship between the location from which a tweet was sent, and the inclusion of location data. Twitter users may want to inform followers that they are tweeting from an impressive or otherwise unusual location - a famous concert venue, or a historic city, or even the site of a terrible natural disaster - indicating to us a relationship between the location to be included and the likelihood of inclusion. Being Missing Not a Random, our options for mitigating location data missingness are few. We could drop the data altogether, but decided that appending it to the contents of the tweet would be an acceptable approach that prevents information loss.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}