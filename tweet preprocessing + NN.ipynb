{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7613 entries, 1 to 10873\n",
      "Data columns (total 4 columns):\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 297.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                       location  \\\n",
       "id                                              \n",
       "48      ablaze                     Birmingham   \n",
       "49      ablaze  Est. September 2012 - Bristol   \n",
       "50      ablaze                         AFRICA   \n",
       "52      ablaze               Philadelphia, PA   \n",
       "53      ablaze                     London, UK   \n",
       "...        ...                            ...   \n",
       "10830  wrecked                            NaN   \n",
       "10831  wrecked              Vancouver, Canada   \n",
       "10832  wrecked                        London    \n",
       "10833  wrecked                        Lincoln   \n",
       "10834  wrecked                            NaN   \n",
       "\n",
       "                                                    text  target  \n",
       "id                                                                \n",
       "48     @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "49     We always try to bring the heavy. #metal #RT h...       0  \n",
       "50     #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "52                    Crying out for more! Set me ablaze       0  \n",
       "53     On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n",
       "...                                                  ...     ...  \n",
       "10830   @jt_ruff23 @cameronhacker and I wrecked you both       0  \n",
       "10831  Three days off from work and they've pretty mu...       0  \n",
       "10832  #FX #forex #trading Cramer: Iger's 3 words tha...       0  \n",
       "10833  @engineshed Great atmosphere at the British Li...       0  \n",
       "10834  Cramer: Iger's 3 words that wrecked Disney's s...       0  \n",
       "\n",
       "[7552 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix as confmat\n",
    "from sklearn.metrics import f1_score as f1\n",
    "# from sklearn.metrics import plot_confusion_matrix as plot_confmat\n",
    "\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "#load dataset and preprocess\n",
    "twt = pd.read_csv('train.csv')\n",
    "twt = twt.set_index('id')\n",
    "twt.shape\n",
    "print(twt.info())\n",
    "twt.loc[~twt['keyword'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "def processing (sentence):\n",
    "    result = sentence.lower() #Lower case \n",
    "    result = re.sub(r'\\d+', '', result) #Removing numbers\n",
    "    result = result.translate(str.maketrans('', '', string.punctuation)) #Remove weird characters\n",
    "    result = result.strip() #Eliminate blanks from begining and end of setences\n",
    "    result = result.split() #Separate into words\n",
    "    result = [w for w in result if not w in stop_words] #Eliminate stop_words\n",
    "    result = [porter.stem(word) for word in result] #Stem Words\n",
    "    return (result)\n",
    "\n",
    "text = list(twt[\"text\"])\n",
    "len(text)\n",
    "\n",
    "prep_text = [processing(i) for i in text]\n",
    "print('{} tweets lowered, tokenized, alphanumerized, stop-stripped, and stemmed.'.format(len(prep_text)))\n",
    "twt['prepped'] = prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prepped</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>[bbcmtd, wholesal, market, ablaz, httptcolhyxe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "      <td>[alway, tri, bring, heavi, metal, rt, httptcoy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[africanbaz, break, newsnigeria, flag, set, ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>[cri, set, ablaz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "      <td>[plu, side, look, sky, last, night, ablaz, htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10826</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>TN</td>\n",
       "      <td>On the bright side I wrecked http://t.co/uEa0t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bright, side, wreck, httptcoueatxrhi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10829</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>#NewcastleuponTyne #UK</td>\n",
       "      <td>@widda16 ... He's gone. You can relax. I thoug...</td>\n",
       "      <td>0</td>\n",
       "      <td>[widda, he, gone, relax, thought, wife, wreck,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[three, day, work, theyv, pretti, much, wreck,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>[fx, forex, trade, cramer, iger, word, wreck, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "      <td>[enginesh, great, atmospher, british, lion, gi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5080 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                       location  \\\n",
       "id                                              \n",
       "48      ablaze                     Birmingham   \n",
       "49      ablaze  Est. September 2012 - Bristol   \n",
       "50      ablaze                         AFRICA   \n",
       "52      ablaze               Philadelphia, PA   \n",
       "53      ablaze                     London, UK   \n",
       "...        ...                            ...   \n",
       "10826  wrecked                             TN   \n",
       "10829  wrecked         #NewcastleuponTyne #UK   \n",
       "10831  wrecked              Vancouver, Canada   \n",
       "10832  wrecked                        London    \n",
       "10833  wrecked                        Lincoln   \n",
       "\n",
       "                                                    text  target  \\\n",
       "id                                                                 \n",
       "48     @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n",
       "49     We always try to bring the heavy. #metal #RT h...       0   \n",
       "50     #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n",
       "52                    Crying out for more! Set me ablaze       0   \n",
       "53     On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   \n",
       "...                                                  ...     ...   \n",
       "10826  On the bright side I wrecked http://t.co/uEa0t...       0   \n",
       "10829  @widda16 ... He's gone. You can relax. I thoug...       0   \n",
       "10831  Three days off from work and they've pretty mu...       0   \n",
       "10832  #FX #forex #trading Cramer: Iger's 3 words tha...       0   \n",
       "10833  @engineshed Great atmosphere at the British Li...       0   \n",
       "\n",
       "                                                 prepped  \n",
       "id                                                        \n",
       "48     [bbcmtd, wholesal, market, ablaz, httptcolhyxe...  \n",
       "49     [alway, tri, bring, heavi, metal, rt, httptcoy...  \n",
       "50     [africanbaz, break, newsnigeria, flag, set, ab...  \n",
       "52                                     [cri, set, ablaz]  \n",
       "53     [plu, side, look, sky, last, night, ablaz, htt...  \n",
       "...                                                  ...  \n",
       "10826             [bright, side, wreck, httptcoueatxrhi]  \n",
       "10829  [widda, he, gone, relax, thought, wife, wreck,...  \n",
       "10831  [three, day, work, theyv, pretti, much, wreck,...  \n",
       "10832  [fx, forex, trade, cramer, iger, word, wreck, ...  \n",
       "10833  [enginesh, great, atmospher, british, lion, gi...  \n",
       "\n",
       "[5080 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twt.loc[twt['keyword'].any()),]\n",
    "twt.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n",
      "5311 tweets vectorized for the training set.\n",
      "2276 tweets vectorized for the dev set.\n",
      "26 tweets were found to be empty.\n"
     ]
    }
   ],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "def tweet_vec (tweet, word2vec):\n",
    "    word_vecs = [word2vec.get_vector(w) for w in tweet if w in word2vec.vocab]\n",
    "#     print(tweet)\n",
    "#     print('Number of words: {}'.format(len(word_vecs)))\n",
    "    if len(word_vecs) >= 1:\n",
    "        return np.stack(word_vecs).mean(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "final_data = []\n",
    "targets = twt['target'].to_numpy()\n",
    "\n",
    "# prep_text[0]\n",
    "# tweet_vec(prep_text[0], word2vec)\n",
    "'''\n",
    "for i in prep_text:\n",
    "    vec = tweet_vec(i,word2vec)\n",
    "    if vec is not None:   \n",
    "        final_data.append(vec)\n",
    "'''      \n",
    "number_empty = 0\n",
    "for x, y in zip(prep_text, targets):\n",
    "    vec = tweet_vec(x, word2vec)\n",
    "    if vec is not None:\n",
    "        final_data.append((vec, y))\n",
    "    else:\n",
    "        number_empty += 1\n",
    "        \n",
    "train_p = 0.70\n",
    "random.shuffle(final_data)\n",
    "\n",
    "# train is final data \n",
    "train = final_data[:round(train_p*len(final_data))]\n",
    "dev = final_data[round(train_p*len(final_data)):]\n",
    "print('Preprocessing complete.\\n{} tweets vectorized for the training set.'.format(len(train)))\n",
    "print('{} tweets vectorized for the dev set.\\n{} tweets were found to be empty.'.format(len(dev), number_empty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5311"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.693\tDev FI: 0.004\n",
      "[1,     2] loss: 0.658\tDev FI: 0.287\n",
      "[2,     1] loss: 0.634\tDev FI: 0.696\n",
      "[2,     2] loss: 0.595\tDev FI: 0.676\n",
      "[3,     1] loss: 0.579\tDev FI: 0.669\n",
      "[3,     2] loss: 0.541\tDev FI: 0.677\n",
      "[4,     1] loss: 0.553\tDev FI: 0.697\n",
      "[4,     2] loss: 0.517\tDev FI: 0.704\n",
      "[5,     1] loss: 0.545\tDev FI: 0.703\n",
      "[5,     2] loss: 0.501\tDev FI: 0.700\n",
      "[6,     1] loss: 0.539\tDev FI: 0.697\n",
      "[6,     2] loss: 0.488\tDev FI: 0.702\n",
      "[7,     1] loss: 0.535\tDev FI: 0.708\n",
      "[7,     2] loss: 0.471\tDev FI: 0.718\n",
      "[8,     1] loss: 0.534\tDev FI: 0.719\n",
      "[8,     2] loss: 0.459\tDev FI: 0.712\n",
      "[9,     1] loss: 0.531\tDev FI: 0.703\n",
      "[9,     2] loss: 0.454\tDev FI: 0.709\n",
      "[10,     1] loss: 0.529\tDev FI: 0.712\n",
      "[10,     2] loss: 0.446\tDev FI: 0.713\n",
      "Finished Training\n",
      "<generator object Module.parameters at 0x000002218C40CC48>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(300,300)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(300, 2)\n",
    "        self.softmax =  nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def get_eval_data(self, data):\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size = 1)\n",
    "\n",
    "        y_stars = []\n",
    "        ys = [vec_targ[1] for vec_targ in data]\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            x, _ = data\n",
    "            # print(x)\n",
    "            output = self.forward(x).detach().numpy()[0]\n",
    "            y_star = np.argmax(output)\n",
    "            #print(y_star)\n",
    "            y_stars.append(y_star)\n",
    "        \n",
    "        return ys, y_stars\n",
    "\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size = 5000)\n",
    "net = Net()\n",
    "# print(net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         print(inputs)\n",
    "#         print(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1 == 0: # print every 2000 mini-batches\n",
    "            ys, y_stars = net.get_eval_data(dev)\n",
    "            print('[%d, %5d] loss: %.3f\\tDev FI: %.3f' % (epoch + 1, i + 1, running_loss, f1(ys, y_stars)))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "print(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7135788894997251"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev[0]\n",
    "ys, y_stars = net.get_eval_data(dev)\n",
    "# confmat(ys, y_stars)\n",
    "f1(ys, y_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263 tweets read from test.csv\n",
      "3263 tweets processed in the test set.\n",
      "0 empty tweets replaced with mean vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# create vector of mean of all word vectors\n",
    "mean_vec = np.zeros((300,1))\n",
    "for vec_targ in train:\n",
    "    mean_vec = np.add(mean_vec, vec_targ[0])\n",
    "mean_vec = mean_vec/len(final_data)\n",
    "\n",
    "# read in test data and preprocess tweets\n",
    "test = pd.read_csv('test.csv')\n",
    "text = list(test[\"text\"])\n",
    "proc_text = [processing(i) for i in text]\n",
    "targets = np.zeros((len(test), 1))\n",
    "print('{} tweets read from test.csv'.format(test.shape[0]))\n",
    "\n",
    "test_data = []\n",
    "counter = 0\n",
    "# replace empty vectors with mean_vec\n",
    "for x, y in zip(prep_text, targets):\n",
    "    vec = tweet_vec(x, word2vec)\n",
    "    if vec is not None:\n",
    "        test_data.append((vec, y))\n",
    "    else:\n",
    "        test_data.append((mean_vec, y))\n",
    "        counter += 0\n",
    "        \n",
    "print('{} tweets processed in the test set.'.format(len(test_data)))\n",
    "print('{} empty tweets replaced with mean vector'.format(counter))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 1)\n",
    "\n",
    "y_stars = []\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    x, y = data\n",
    "    #print(x.dtype)\n",
    "    #x_dub = torch.Tensor(x, dtype = 'double')\n",
    "    #print(x.dtype)\n",
    "    output = net.forward(x.float()).detach().numpy()[0]\n",
    "    y_star = np.argmax(output)\n",
    "    #print(y_star)\n",
    "    y_stars.append(y_star)\n",
    "\n",
    "# create columns for submission data\n",
    "id = test['id'].to_numpy()\n",
    "target = y_stars\n",
    "\n",
    "# create df with submission data and write to csv\n",
    "submission = pd.DataFrame({'id': id, 'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.set_index('id')\n",
    "submission.to_csv('submission_3_19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7613 bag-of-words vectorizations.\n",
      "Naive Bayes training complete.\n",
      "Naive Bayes dev performance 0.7468085106382979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create raw bow text and labels, shuffle\n",
    "bow = twt['text'].to_numpy()\n",
    "labels = twt['target'].to_numpy()\n",
    "\n",
    "shuffle_idx = [x for x in range(len(bow))]\n",
    "random.shuffle(shuffle_idx)\n",
    "\n",
    "bow = bow[shuffle_idx]\n",
    "labels = labels[shuffle_idx]\n",
    "\n",
    "# train bag-of-words\n",
    "count_vect = CountVectorizer()\n",
    "x_bow = count_vect.fit(bow)\n",
    "\n",
    "# create train dev split\n",
    "train_bow = bow[:round(train_p*len(bow))]\n",
    "dev_bow = bow[round(train_p*len(bow)):]\n",
    "train_bow = count_vect.transform(train_bow)\n",
    "dev_bow = count_vect.transform(dev_bow)\n",
    "\n",
    "train_bow_labels = labels[:round(train_p*len(bow))]\n",
    "dev_bow_lables = labels[round(train_p*len(bow)):]\n",
    "\n",
    "print('Created {} bag-of-words vectorizations.'.format(train_bow.shape[0] + dev_bow.shape[0]))\n",
    "clf = MultinomialNB().fit(train_bow, train_bow_labels)\n",
    "print('Naive Bayes training complete.')\n",
    "\n",
    "# get dev performances\n",
    "ys = clf.predict(dev_bow)\n",
    "\n",
    "# dev performance\n",
    "print('Naive Bayes dev performance {}'.format(f1(ys, dev_bow_lables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
